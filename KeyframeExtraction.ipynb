{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as gb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_INDEX = None\n",
    "CLASS_INDEX_PATH = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x, dim_ordering='default'):\n",
    "    if dim_ordering == 'default':\n",
    "        dim_ordering = K.image_data_format()\n",
    "    x = x.astype(float)\n",
    "    if dim_ordering == 'th':\n",
    "        x[:, 0, :, :] -= 103.939\n",
    "        x[:, 1, :, :] -= 116.779\n",
    "        x[:, 2, :, :] -= 123.68\n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[:, ::-1, :, :]\n",
    "    else:\n",
    "        x[:, :, :, 0] -= 103.939\n",
    "        x[:, :, :, 1] -= 116.779\n",
    "        x[:, :, :, 2] -= 123.68\n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[:, :, :, ::-1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(preds, top=5):\n",
    "    global CLASS_INDEX\n",
    "    if len(preds.shape) != 2 or preds.shape[1] != 512:\n",
    "        raise ValueError('`decode_predictions` expects '\n",
    "                         'a batch of predictions '\n",
    "                         '(i.e. a 2D array of shape (samples, 1000)). '\n",
    "                         'Found array with shape: ' + str(preds.shape))\n",
    "    if CLASS_INDEX is None:\n",
    "        fpath = get_file('imagenet_class_index.json',\n",
    "                         CLASS_INDEX_PATH,\n",
    "                         cache_subdir='models')\n",
    "        CLASS_INDEX = json.load(open(fpath))\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        top_indices = pred.argsort()[-top:][::-1]\n",
    "        result = [tuple(CLASS_INDEX[str(i)]) + (pred[i],) for i in top_indices]\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "from keras import backend\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.engine import training\n",
    "from keras.layers import VersionAwareLayers\n",
    "from keras.utils import data_utils\n",
    "from keras.utils import layer_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "\n",
    "# isort: off\n",
    "from tensorflow.python.util.tf_export import keras_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\",\n",
    "):\n",
    "    if not (weights in {\"imagenet\", None} or tf.io.gfile.exists(weights)):\n",
    "        raise ValueError(\n",
    "            \"The `weights` argument should be either \"\n",
    "            \"`None` (random initialization), `imagenet` \"\n",
    "            \"(pre-training on ImageNet), \"\n",
    "            \"or the path to the weights file to be loaded.  Received: \"\n",
    "            f\"weights={weights}\"\n",
    "        )\n",
    "\n",
    "    if weights == \"imagenet\" and include_top and classes != 1000:\n",
    "        raise ValueError(\n",
    "            'If using `weights` as `\"imagenet\"` with `include_top` '\n",
    "            \"as true, `classes` should be 1000.  \"\n",
    "            f\"Received `classes={classes}`\"\n",
    "        )\n",
    "    # Determine proper input shape\n",
    "    input_shape = imagenet_utils.obtain_input_shape(\n",
    "        input_shape,\n",
    "        default_size=224,\n",
    "        min_size=32,\n",
    "        data_format=backend.image_data_format(),\n",
    "        require_flatten=include_top,\n",
    "        weights=weights,\n",
    "    )\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    # Block 1\n",
    "    x = Conv2D(\n",
    "        64, (3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv1\"\n",
    "    )(img_input)\n",
    "    x = Conv2D(\n",
    "        64, (3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv2\"\n",
    "    )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block1_pool\")(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(\n",
    "        128, (3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv1\"\n",
    "    )(x)\n",
    "    x = Conv2D(\n",
    "        128, (3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv2\"\n",
    "    )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block2_pool\")(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(\n",
    "        256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv1\"\n",
    "    )(x)\n",
    "    x = Conv2D(\n",
    "        256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv2\"\n",
    "    )(x)\n",
    "    x = Conv2D(\n",
    "        256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv3\"\n",
    "    )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block3_pool\")(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv1\"\n",
    "    )(x)\n",
    "    x = Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv2\"\n",
    "    )(x)\n",
    "    x = Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv3\"\n",
    "    )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block4_pool\")(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv1\"\n",
    "    )(x)\n",
    "    x = Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv2\"\n",
    "    )(x)\n",
    "    x = Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv3\"\n",
    "    )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block5_pool\")(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = Flatten(name=\"flatten\")(x)\n",
    "        x = Dense(4096, activation=\"relu\", name=\"fc1\")(x)\n",
    "        x = Dense(4096, activation=\"relu\", name=\"fc2\")(x)\n",
    "\n",
    "        imagenet_utils.validate_activation(classifier_activation, weights)\n",
    "        x = Dense(\n",
    "            classes, activation=classifier_activation, name=\"predictions\"\n",
    "        )(x)\n",
    "    else:\n",
    "        if pooling == \"avg\":\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == \"max\":\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = layer_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = training.Model(inputs, x, name=\"vgg16\")\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == \"imagenet\":\n",
    "        if include_top:\n",
    "            weights_path = data_utils.get_file(\n",
    "                \"vgg16_weights_tf_dim_ordering_tf_kernels.h5\",\n",
    "                TF_WEIGHTS_PATH,\n",
    "                cache_subdir=\"models\",\n",
    "                file_hash=\"64373286793e3c8b2b4e3219cbf3544b\",\n",
    "            )\n",
    "        else:\n",
    "            weights_path = data_utils.get_file(\n",
    "                \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n",
    "                TF_WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir=\"models\",\n",
    "                file_hash=\"6d6bbae143d832006294945121d1f1fc\",\n",
    "            )\n",
    "        model.load_weights(weights_path)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet',include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: (1, 224, 224, 3)\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "(1, 512)\n",
      "Predicted: Afghan_hound\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    img_path = 'elephant.jpg'\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((224, 224))\n",
    "    x = np.array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    print('Input image shape:', x.shape)\n",
    "\n",
    "    preds = model.predict(x)\n",
    "    print(preds.shape)\n",
    "    pred = decode_predictions(preds)\n",
    "    print('Predicted:', pred[0][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyframes extraction from videos using VSUMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts the embeddings of the frames from the model and is inserted into an array called `features` to be fed to the kmeans clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_feat(frames_raw, frames_shape):\n",
    "    frames=[]\n",
    "    pca = PCA(n_components=frames_shape)\n",
    "    for im in frames_raw:\n",
    "        #print im.shape\n",
    "        im = cv2.resize(im, (224, 224)).astype(np.float32)\n",
    "        im[:,:,0] -= 103.939\n",
    "        im[:,:,1] -= 116.779\n",
    "        im[:,:,2] -= 123.68\n",
    "        # print im.shape\n",
    "        im = np.expand_dims(im, axis=0)\n",
    "        #print im.shape\n",
    "        frames.append(np.asarray(im))\n",
    "    frames = np.array(frames)\n",
    "    #print frames.shape\n",
    "\n",
    "    model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    i = 0\n",
    "    features = np.ndarray((frames.shape[0], 512), dtype=np.float32)\n",
    "    for x in frames:\n",
    "        print(model.predict(x).shape)\n",
    "        #print x.shape\n",
    "        features[i,:] = model.predict(x)\n",
    "        i+=1\n",
    "    return pca.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets coloured Histogram which is generally used in VSUMM but not used in this case as we cluster the embeddings instead of the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_hist(frames_raw, num_bins):\n",
    "    print (\"Generating linear Histrograms using OpenCV\")\n",
    "    channels=['b','g','r']\n",
    "    \n",
    "    hist=[]\n",
    "    for frame in frames_raw:\n",
    "        feature_value=[cv2.calcHist([frame],[i],None,[int(num_bins)],[0,256]) for i,col in enumerate(channels)]\n",
    "        hist.append(np.asarray(feature_value).flatten())\n",
    "    \n",
    "    hist=np.asarray(hist)\n",
    "    #print \"Done generating!\"\n",
    "    print (\"Shape of histogram: \" + str(hist.shape))\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic VSUMM to test with different features\n",
    "# k means clustering to generate video summary\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "# k-means\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import IPython.display \n",
    "from IPython.display import Video, Image, display\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the keyframes to respective created paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame chosen every k frames\n",
    "sampling_rate= 7\n",
    "\n",
    "# percent of video for summary\n",
    "percent= 2\n",
    "\n",
    "# globalizing\n",
    "num_centroids=0\n",
    "SaveFrames = False\n",
    "SaveKeyFrames = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main() function to execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global num_bins, sampling_rate, num_centroids, percent\n",
    "    #for folder in  os.listdir(\"faceforensics\") :\n",
    "        #if folder == 'manipulated_sequences':\n",
    "    files_fake = gb.glob(pathname= str( 'faceforensics/manipulated_sequences/Deepfakes/c23/videos' + '/*.mp4'))\n",
    "    #else:\n",
    "    files_real = gb.glob(pathname= str('faceforensics/original_sequences/youtube/c23/videos' + '/*.mp4'))\n",
    "    #In case code is stuck start with the ending video number and by somehow filtering the string and change j to the last frame value + 1 \n",
    "    j = 8972          \n",
    "    for file_fake, file_real in zip(files_fake, files_real):\n",
    "        print(file_fake, file_real)\n",
    "        print (\"Opening fake video!\")\n",
    "        capture_fake = cv2.VideoCapture(file_fake)\n",
    "        print (\"Video opened\\nChoosing frames_fake\")\n",
    "        Video(file_fake, width = 100)\n",
    "    \n",
    "        #choosing the subset of frames_fake from which video summary will be generated\n",
    "        frames_fake = []\n",
    "        i=0\n",
    "        while(capture_fake.isOpened()):\n",
    "            if i % int(sampling_rate) == 0:\n",
    "                capture_fake.set(1,i)\n",
    "                # print i\n",
    "                ret, frame_fake = capture_fake.read()\n",
    "                if len(frames_fake) == 512 or frame_fake is None:\n",
    "                    break\n",
    "                #im = np.expand_dims(im, axis=0) #convert to (1, width, height, depth)\n",
    "                # print frame.shape\n",
    "                frames_fake.append(np.asarray(frame_fake))\n",
    "            i+=1\n",
    "        frames_fake = np.array(frames_fake)#convert to (num_frames_fake, width, height, depth)\n",
    "        print (\"frames_fake chosen\")\n",
    "        print (\"Length of fake video %d\" % frames_fake.shape[0])\n",
    "        # REPLACE WITH APPROPRIATE FEATURES\n",
    "\n",
    "        features = get_cnn_feat(frames_fake, frames_fake.shape[0])\n",
    "        print (\"Shape of fake frame features \" + str(features.shape))\n",
    "\n",
    "        # clustering: defaults to using the features\n",
    "        print (\"Clustering\")\n",
    "\n",
    "        # converting percentage to actual number\n",
    "        num_centroids=int(percent*frames_fake.shape[0]*sampling_rate/100)   \n",
    "\n",
    "\t    # choose number of centroids for clustering from user required frames_fake (specified in GT folder for each video)\n",
    "        if percent == -1:\n",
    "            video_address = file_fake.split('/')\n",
    "            gt_file = video_address[len(video_address) - 1].split('.')[0] + '.mat'\n",
    "            video_address[len(video_address) - 1] = gt_file\n",
    "            video_address[len(video_address) - 2] = 'GT'\n",
    "            gt_file = '/'.join(video_address)\n",
    "            num_frames_fake = int(scipy.io.loadmat(gt_file).get('user_score').shape[0])\n",
    "\n",
    "        if len(frames_fake) < num_centroids:\n",
    "            print (\"Samples too less to generate such a large summary\")\n",
    "            print (\"Changing to maximum possible centroids\")\n",
    "            num_centroids=frames_fake.shape[0]\n",
    "        #kmeans = GaussianMixture(n_components=num_centroid)\n",
    "        kmeans = KMeans(n_clusters=num_centroids)\n",
    "        print (\"Done Clustering!\")\n",
    "\n",
    "        print (\"Generating summary frames\")\n",
    "        summary_frames_fake=[]\n",
    "\n",
    "        # transforms into cluster-distance space (n_cluster dimensional)\n",
    "        \"\"\"feature_transform= kmeans.fit_transform(features)\n",
    "        frame_indices=[]\n",
    "        for cluster in range(feature_transform.shape[1]):\n",
    "            print (\"Frame number: %d\" % (np.argmin(feature_transform.T[cluster])*sampling_rate))\n",
    "            frame_indices.append(np.argmin(feature_transform.T[cluster]))\"\"\"\n",
    "        feat = kmeans.fit_predict(features)\n",
    "        #plt.scatter(features[:, 0], features[:, 1], c = feat, s = 40, cmap = 'viridis')    \n",
    "        frame_indices = []\n",
    "        for cluster_center in kmeans.cluster_centers_:  # Loop over the cluster centers\n",
    "            distances_to_center = np.linalg.norm(features - cluster_center, axis=1)  # Distances to the center\n",
    "            closest_point_index = np.argmin(distances_to_center)  # Index of the closest data point\n",
    "            frame_indices.append(closest_point_index)  # Choose the frame closest to the cluster center\n",
    "    \n",
    "        print(sorted(frame_indices))\n",
    "\t    # frames_fake generated in sequence from original video\n",
    "        frame_indices=sorted(frame_indices)\n",
    "        summary_frames_fake=[frames_fake[i] for i in frame_indices]\n",
    "        print (\"Generated summary\")\n",
    "        \n",
    "        print (\"Opening original video!\")\n",
    "        capture_real = cv2.VideoCapture(file_real)\n",
    "        print (\"Video opened\\nChoosing frames_real\")\n",
    "        print(Video(file_real, width = 100))\n",
    "        \n",
    "        frames_real = []\n",
    "        k = 0\n",
    "        while(capture_real.isOpened()):\n",
    "            if k % int(sampling_rate) == 0:\n",
    "                capture_real.set(1,k)\n",
    "                ret, frame_real = capture_real.read()\n",
    "                if len(frames_real) == 512 or frame_real is None:\n",
    "                    break\n",
    "                #im = np.expand_dims(im, axis=0) #convert to (1, width, height, depth)\n",
    "                # print frame.shape\n",
    "                frames_real.append(np.asarray(frame_real))\n",
    "            k+=1\n",
    "        frames_real = np.array(frames_real)#convert to (num_frames_real, width, height, depth)\n",
    "        print (\"frames_real chosen\")\n",
    "        print (\"Length of real video %d\" % frames_real.shape[0])\n",
    "        \n",
    "        for i in frame_indices:\n",
    "            #if folder == 'manipulated_sequences':\n",
    "            frames_fake[i] = cv2.cvtColor(frames_fake[i], cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(os.path.join('Training_images/fake', f'frame_{j}.jpg'), frames_fake[i])\n",
    "            #else:\n",
    "            cv2.imwrite(os.path.join('Training_images/real', f'frame_{j}.jpg'), frames_real[i])  \n",
    "            j += 1 \n",
    "        print(f\"Length of selected Keyframes_fake:{len(frame_indices)}\")        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
