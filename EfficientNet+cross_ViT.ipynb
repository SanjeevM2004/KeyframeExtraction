{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yDkeVMoH6U_m"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import dlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "expaqnlLE9bf"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MJ5XV0eTE9d6"
      },
      "outputs": [],
      "source": [
        "dataset_url=\"https://www.kaggle.com/datasets/sorokin/faceforensics\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DX042U7GE9gg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "import os\n",
        "import glob as gb\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import time\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "from torch import einsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8F4SYGGcE9la"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ShNv1vHFE9oF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading faceforensics.zip to .\\faceforensics\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8.40G/8.40G [12:49<00:00, 11.7MB/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "od.download(dataset_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R0WmEDlBE9td"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SSbDMcrqE9v1"
      },
      "outputs": [],
      "source": [
        "fake = 'faceforensics/manipulated_sequences/Deepfakes/c23/videos'\n",
        "original = 'faceforensics/original_sequences/youtube/c23/videos'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rSWN1D0yE9zt"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"Training_images\")\n",
        "os.mkdir(\"Training_images/fake\")\n",
        "os.mkdir(\"Training_images/real\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "evxUqyKkE91u"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RPbZUovrE93b"
      },
      "outputs": [],
      "source": [
        "x_real = []\n",
        "y_real = []\n",
        "x_fake=[]\n",
        "y_fake=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lE9dGJ0wE980"
      },
      "outputs": [],
      "source": [
        "for folder in  os.listdir(\"Training_images\") :\n",
        "    files = gb.glob(pathname= str( \"Training_images/\" + folder + '/*.jpg'))\n",
        "    for file in files:\n",
        "      image = cv2.imread(file)\n",
        "      if folder == \"fake\":\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "      image = (torch.tensor(np.array(cv2.resize(image,(224,224)))).permute(2,0,1) )/255\n",
        "      if folder==\"real\":\n",
        "        y_real.append(0)\n",
        "        x_real.append(image)\n",
        "      else:\n",
        "        y_fake.append(1)\n",
        "        x_fake.append(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0To_2h8cE-AY"
      },
      "outputs": [],
      "source": [
        "if len(x_real)<len(x_fake):\n",
        " x_fake=random.sample(x_fake,k=len(x_real))\n",
        " y_fake=random.sample(y_fake,k=len(y_real))\n",
        "else:\n",
        " x_real=random.sample(x_real,len(x_fake))\n",
        " y_real=random.sample(y_real,len(y_fake))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kiH8HPphFh8c"
      },
      "outputs": [],
      "source": [
        "x_real,x_real_val = torch.utils.data.random_split(x_real, [int(0.7*len(x_real)), len(x_real)-int(0.7*len(x_real))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9JpavZNfFh_W"
      },
      "outputs": [],
      "source": [
        "x_fake,x_fake_val = torch.utils.data.random_split(x_fake, [int(0.7*len(x_fake)), len(x_fake)-int(0.7*len(x_fake))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PkA8kMqRNXPX"
      },
      "outputs": [],
      "source": [
        "y_real,y_real_val = torch.utils.data.random_split(y_real, [int(0.7*len(y_real)), len(y_real)-int(0.7*len(y_real))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xsslE0CCFiEK"
      },
      "outputs": [],
      "source": [
        "y_fake,y_fake_val = torch.utils.data.random_split(y_fake, [int(0.7*len(y_fake)), len(y_fake)-int(0.7*len(y_fake))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nBKpST5eFiHu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import ceil\n",
        "\n",
        "base_model = [\n",
        "    # expand_ratio, channels, repeats, stride, kernel_size\n",
        "    [1, 16, 1, 1, 3],\n",
        "    [6, 24, 2, 2, 3],\n",
        "    [6, 40, 2, 2, 5],\n",
        "    [6, 80, 3, 2, 3],\n",
        "    [6, 112, 3, 1, 5],\n",
        "    [6, 192, 4, 2, 5],\n",
        "    [6, 320, 1, 1, 3],\n",
        "]\n",
        "\n",
        "phi_values = {\n",
        "    # tuple of: (phi_value, resolution, drop_rate)\n",
        "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
        "    \"b1\": (0.5, 240, 0.2),\n",
        "    \"b2\": (1, 260, 0.3),\n",
        "    \"b3\": (2, 300, 0.3),\n",
        "    \"b4\": (3, 380, 0.4),\n",
        "    \"b5\": (4, 456, 0.4),\n",
        "    \"b6\": (5, 528, 0.5),\n",
        "    \"b7\": (6, 600, 0.5),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OJ-fYSEbFtJy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in c:\\users\\sanje\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.7.1)\n",
            "Requirement already satisfied: torch in c:\\users\\sanje\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from efficientnet_pytorch) (2.0.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\sanje\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->efficientnet_pytorch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\sanje\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->efficientnet_pytorch) (4.6.3)\n",
            "Requirement already satisfied: sympy in c:\\users\\sanje\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->efficientnet_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\sanje\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sanje\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sanje\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sanje\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ],
      "source": [
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "b0=EfficientNet.from_pretrained('efficientnet-b0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bOwA89TsFysU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_k = nn.Linear(dim, inner_dim , bias=False)\n",
        "        self.to_v = nn.Linear(dim, inner_dim , bias = False)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x_qkv):\n",
        "        b, n, _, h = *x_qkv.shape, self.heads\n",
        "\n",
        "        k = self.to_k(x_qkv)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h = h)\n",
        "\n",
        "        v = self.to_v(x_qkv)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h = h)\n",
        "\n",
        "        q = self.to_q(x_qkv[:, 0].unsqueeze(1))\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h = h)\n",
        "\n",
        "\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCqgoUqg1zDi",
        "outputId": "47cf3662-f0cf-4593-e8d9-d3eadb08dfb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in c:\\users\\sanje\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "!pip install einops\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiScaleTransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, small_dim = 96, small_depth = 4, small_heads =3, small_dim_head = 32, small_mlp_dim = 384,\n",
        "                 large_dim = 192, large_depth = 1, large_heads = 3, large_dim_head = 64, large_mlp_dim = 768,\n",
        "                 cross_attn_depth = 1, cross_attn_heads = 3, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.transformer_enc_small = Transformer(small_dim, small_depth, small_heads, small_dim_head, small_mlp_dim)\n",
        "        self.transformer_enc_large = Transformer(large_dim, large_depth, large_heads, large_dim_head, large_mlp_dim)\n",
        "\n",
        "        self.cross_attn_layers = nn.ModuleList([])\n",
        "        for _ in range(cross_attn_depth):\n",
        "            self.cross_attn_layers.append(nn.ModuleList([\n",
        "                nn.Linear(small_dim, large_dim),\n",
        "                nn.Linear(large_dim, small_dim),\n",
        "                PreNorm(large_dim, CrossAttention(large_dim, heads = cross_attn_heads, dim_head = large_dim_head, dropout = dropout)),\n",
        "                nn.Linear(large_dim, small_dim),\n",
        "                nn.Linear(small_dim, large_dim),\n",
        "                PreNorm(small_dim, CrossAttention(small_dim, heads = cross_attn_heads, dim_head = small_dim_head, dropout = dropout)),\n",
        "            ]))\n",
        "\n",
        "    def forward(self, xs, xl):\n",
        "\n",
        "        xs = self.transformer_enc_small(xs)\n",
        "        xl = self.transformer_enc_large(xl)\n",
        "\n",
        "        for f_sl, g_ls, cross_attn_s, f_ls, g_sl, cross_attn_l in self.cross_attn_layers:\n",
        "            small_class = xs[:, 0]\n",
        "            x_small = xs[:, 1:]\n",
        "            large_class = xl[:, 0]\n",
        "            x_large = xl[:, 1:]\n",
        "\n",
        "            # Cross Attn for Large Patch\n",
        "\n",
        "            cal_q = f_ls(large_class.unsqueeze(1))\n",
        "            cal_qkv = torch.cat((cal_q, x_small), dim=1)\n",
        "            cal_out = cal_q + cross_attn_l(cal_qkv)\n",
        "            cal_out = g_sl(cal_out)\n",
        "            xl = torch.cat((cal_out, x_large), dim=1)\n",
        "\n",
        "            # Cross Attn for Smaller Patch\n",
        "            cal_q = f_sl(small_class.unsqueeze(1))\n",
        "            cal_qkv = torch.cat((cal_q, x_large), dim=1)\n",
        "            cal_out = cal_q + cross_attn_s(cal_qkv)\n",
        "            cal_out = g_ls(cal_out)\n",
        "            xs = torch.cat((cal_out, x_small), dim=1)\n",
        "\n",
        "        return xs, xl\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, image_size, channels, num_classes, patch_size_small = 14, patch_size_large = 16, small_dim = 96,\n",
        "                 large_dim = 192, small_depth = 1, large_depth = 4, cross_attn_depth = 1, multi_scale_enc_depth = 3,\n",
        "                 heads = 3, pool = 'cls', dropout = 0., emb_dropout = 0., scale_dim = 4):\n",
        "        super().__init__()\n",
        "\n",
        "        assert image_size % patch_size_small == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches_small = (image_size // patch_size_small) ** 2\n",
        "        patch_dim_small = channels * patch_size_small ** 2\n",
        "\n",
        "        assert image_size % patch_size_large == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        num_patches_large = (image_size // patch_size_large) ** 2\n",
        "        patch_dim_large = channels * patch_size_large ** 2\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "\n",
        "        self.to_patch_embedding_small = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size_small, p2 = patch_size_small),\n",
        "            nn.Linear(patch_dim_small, small_dim),\n",
        "        )\n",
        "\n",
        "        self.to_patch_embedding_large = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size_large, p2=patch_size_large),\n",
        "            nn.Linear(patch_dim_large, large_dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding_small = nn.Parameter(torch.randn(1, num_patches_small + 1, small_dim))\n",
        "        self.cls_token_small = nn.Parameter(torch.randn(1, 1, small_dim))\n",
        "        self.dropout_small = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.pos_embedding_large = nn.Parameter(torch.randn(1, num_patches_large + 1, large_dim))\n",
        "        self.cls_token_large = nn.Parameter(torch.randn(1, 1, large_dim))\n",
        "        self.dropout_large = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.multi_scale_transformers = nn.ModuleList([])\n",
        "        for _ in range(multi_scale_enc_depth):\n",
        "            self.multi_scale_transformers.append(MultiScaleTransformerEncoder(small_dim=small_dim, small_depth=small_depth,\n",
        "                                                                              small_heads=heads, small_dim_head=small_dim//heads,\n",
        "                                                                              small_mlp_dim=small_dim*scale_dim,\n",
        "                                                                              large_dim=large_dim, large_depth=large_depth,\n",
        "                                                                              large_heads=heads, large_dim_head=large_dim//heads,\n",
        "                                                                              large_mlp_dim=large_dim*scale_dim,\n",
        "                                                                              cross_attn_depth=cross_attn_depth, cross_attn_heads=heads,\n",
        "                                                                              dropout=dropout))\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head_small = nn.Sequential(\n",
        "            nn.LayerNorm(small_dim),\n",
        "            nn.Linear(small_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.mlp_head_large = nn.Sequential(\n",
        "            nn.LayerNorm(large_dim),\n",
        "            nn.Linear(large_dim, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "\n",
        "        xs = self.to_patch_embedding_small(img)\n",
        "        b, n, _ = xs.shape\n",
        "\n",
        "        cls_token_small = repeat(self.cls_token_small, '() n d -> b n d', b = b)\n",
        "        xs = torch.cat((cls_token_small, xs), dim=1)\n",
        "        xs += self.pos_embedding_small[:, :(n + 1)]\n",
        "        xs = self.dropout_small(xs)\n",
        "\n",
        "        xl = self.to_patch_embedding_large(img)\n",
        "        b, n, _ = xl.shape\n",
        "\n",
        "        cls_token_large = repeat(self.cls_token_large, '() n d -> b n d', b=b)\n",
        "        xl = torch.cat((cls_token_large, xl), dim=1)\n",
        "        xl += self.pos_embedding_large[:, :(n + 1)]\n",
        "        xl = self.dropout_large(xl)\n",
        "\n",
        "        for multi_scale_transformer in self.multi_scale_transformers:\n",
        "            xs, xl = multi_scale_transformer(xs, xl)\n",
        "\n",
        "        xs = xs.mean(dim = 1) if self.pool == 'mean' else xs[:, 0]\n",
        "        xl = xl.mean(dim = 1) if self.pool == 'mean' else xl[:, 0]\n",
        "\n",
        "        xs = self.mlp_head_small(xs)\n",
        "        xl = self.mlp_head_large(xl)\n",
        "        x = xs + xl\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cEEc99AESHS0"
      },
      "outputs": [],
      "source": [
        "# import pyyaml module\n",
        "import yaml\n",
        "from yaml.loader import SafeLoader\n",
        "\n",
        "# Open the file and load the file\n",
        "with open('config.yaml') as f:\n",
        "    config = yaml.load(f, Loader=SafeLoader)\n",
        "    #print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY0e-Ndj8jzm",
        "outputId": "7dd051d8-6adf-495e-c7d6-14456ab30e09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'EfficientNet-PyTorch' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/lukemelas/EfficientNet-PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scFHYldf9GVT",
        "outputId": "bf9adb36-04c6-4df6-ef0e-1110816bd1fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 3] The system cannot find the path specified: '/content/EfficientNet-PyTorch'\n",
            "c:\\Data\\AIClub\n"
          ]
        }
      ],
      "source": [
        "%cd /content/EfficientNet-PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P23ZvXsNXtaz"
      },
      "outputs": [],
      "source": [
        "\"\"\"utils.py - Helper functions for building the model and for loading model parameters.\n",
        "   These helper functions are built to mirror those in the official TensorFlow implementation.\n",
        "\"\"\"\n",
        "\n",
        "# Author: lukemelas (github username)\n",
        "# Github repo: https://github.com/lukemelas/EfficientNet-PyTorch\n",
        "# With adjustments and added comments by workingcoder (github username).\n",
        "\n",
        "import re\n",
        "import math\n",
        "import collections\n",
        "from functools import partial\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import model_zoo\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Help functions for model architecture\n",
        "################################################################################\n",
        "\n",
        "# GlobalParams and BlockArgs: Two namedtuples\n",
        "# Swish and MemoryEfficientSwish: Two implementations of the method\n",
        "# round_filters and round_repeats:\n",
        "#     Functions to calculate params for scaling model width and depth ! ! !\n",
        "# get_width_and_height_from_size and calculate_output_image_size\n",
        "# drop_connect: A structural design\n",
        "# get_same_padding_conv2d:\n",
        "#     Conv2dDynamicSamePadding\n",
        "#     Conv2dStaticSamePadding\n",
        "# get_same_padding_maxPool2d:\n",
        "#     MaxPool2dDynamicSamePadding\n",
        "#     MaxPool2dStaticSamePadding\n",
        "#     It's an additional function, not used in EfficientNet,\n",
        "#     but can be used in other model (such as EfficientDet).\n",
        "\n",
        "# Parameters for the entire model (stem, all blocks, and head)\n",
        "GlobalParams = collections.namedtuple('GlobalParams', [\n",
        "    'width_coefficient', 'depth_coefficient', 'image_size', 'dropout_rate',\n",
        "    'num_classes', 'batch_norm_momentum', 'batch_norm_epsilon',\n",
        "    'drop_connect_rate', 'depth_divisor', 'min_depth', 'include_top'])\n",
        "\n",
        "# Parameters for an individual model block\n",
        "BlockArgs = collections.namedtuple('BlockArgs', [\n",
        "    'num_repeat', 'kernel_size', 'stride', 'expand_ratio',\n",
        "    'input_filters', 'output_filters', 'se_ratio', 'id_skip'])\n",
        "\n",
        "# Set GlobalParams and BlockArgs's defaults\n",
        "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
        "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
        "\n",
        "# Swish activation function\n",
        "if hasattr(nn, 'SiLU'):\n",
        "    Swish = nn.SiLU\n",
        "else:\n",
        "    # For compatibility with old PyTorch versions\n",
        "    class Swish(nn.Module):\n",
        "        def forward(self, x):\n",
        "            return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "# A memory-efficient implementation of Swish function\n",
        "class SwishImplementation(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, i):\n",
        "        result = i * torch.sigmoid(i)\n",
        "        ctx.save_for_backward(i)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        i = ctx.saved_tensors[0]\n",
        "        sigmoid_i = torch.sigmoid(i)\n",
        "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
        "\n",
        "\n",
        "class MemoryEfficientSwish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return SwishImplementation.apply(x)\n",
        "\n",
        "\n",
        "def round_filters(filters, global_params):\n",
        "    \"\"\"Calculate and round number of filters based on width multiplier.\n",
        "       Use width_coefficient, depth_divisor and min_depth of global_params.\n",
        "\n",
        "    Args:\n",
        "        filters (int): Filters number to be calculated.\n",
        "        global_params (namedtuple): Global params of the model.\n",
        "\n",
        "    Returns:\n",
        "        new_filters: New filters number after calculating.\n",
        "    \"\"\"\n",
        "    multiplier = global_params.width_coefficient\n",
        "    if not multiplier:\n",
        "        return filters\n",
        "    # TODO: modify the params names.\n",
        "    #       maybe the names (width_divisor,min_width)\n",
        "    #       are more suitable than (depth_divisor,min_depth).\n",
        "    divisor = global_params.depth_divisor\n",
        "    min_depth = global_params.min_depth\n",
        "    filters *= multiplier\n",
        "    min_depth = min_depth or divisor  # pay attention to this line when using min_depth\n",
        "    # follow the formula transferred from official TensorFlow implementation\n",
        "    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
        "    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n",
        "        new_filters += divisor\n",
        "    return int(new_filters)\n",
        "\n",
        "\n",
        "def round_repeats(repeats, global_params):\n",
        "    \"\"\"Calculate module's repeat number of a block based on depth multiplier.\n",
        "       Use depth_coefficient of global_params.\n",
        "\n",
        "    Args:\n",
        "        repeats (int): num_repeat to be calculated.\n",
        "        global_params (namedtuple): Global params of the model.\n",
        "\n",
        "    Returns:\n",
        "        new repeat: New repeat number after calculating.\n",
        "    \"\"\"\n",
        "    multiplier = global_params.depth_coefficient\n",
        "    if not multiplier:\n",
        "        return repeats\n",
        "    # follow the formula transferred from official TensorFlow implementation\n",
        "    return int(math.ceil(multiplier * repeats))\n",
        "\n",
        "\n",
        "def drop_connect(inputs, p, training):\n",
        "    \"\"\"Drop connect.\n",
        "\n",
        "    Args:\n",
        "        input (tensor: BCWH): Input of this structure.\n",
        "        p (float: 0.0~1.0): Probability of drop connection.\n",
        "        training (bool): The running mode.\n",
        "\n",
        "    Returns:\n",
        "        output: Output after drop connection.\n",
        "    \"\"\"\n",
        "    assert 0 <= p <= 1, 'p must be in range of [0,1]'\n",
        "\n",
        "    if not training:\n",
        "        return inputs\n",
        "\n",
        "    batch_size = inputs.shape[0]\n",
        "    keep_prob = 1 - p\n",
        "\n",
        "    # generate binary_tensor mask according to probability (p for 0, 1-p for 1)\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
        "    binary_tensor = torch.floor(random_tensor)\n",
        "\n",
        "    output = inputs / keep_prob * binary_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "def get_width_and_height_from_size(x):\n",
        "    \"\"\"Obtain height and width from x.\n",
        "\n",
        "    Args:\n",
        "        x (int, tuple or list): Data size.\n",
        "\n",
        "    Returns:\n",
        "        size: A tuple or list (H,W).\n",
        "    \"\"\"\n",
        "    if isinstance(x, int):\n",
        "        return x, x\n",
        "    if isinstance(x, list) or isinstance(x, tuple):\n",
        "        return x\n",
        "    else:\n",
        "        raise TypeError()\n",
        "\n",
        "\n",
        "def calculate_output_image_size(input_image_size, stride):\n",
        "    \"\"\"Calculates the output image size when using Conv2dSamePadding with a stride.\n",
        "       Necessary for static padding. Thanks to mannatsingh for pointing this out.\n",
        "\n",
        "    Args:\n",
        "        input_image_size (int, tuple or list): Size of input image.\n",
        "        stride (int, tuple or list): Conv2d operation's stride.\n",
        "\n",
        "    Returns:\n",
        "        output_image_size: A list [H,W].\n",
        "    \"\"\"\n",
        "    if input_image_size is None:\n",
        "        return None\n",
        "    image_height, image_width = get_width_and_height_from_size(input_image_size)\n",
        "    stride = stride if isinstance(stride, int) else stride[0]\n",
        "    image_height = int(math.ceil(image_height / stride))\n",
        "    image_width = int(math.ceil(image_width / stride))\n",
        "    return [image_height, image_width]\n",
        "\n",
        "\n",
        "# Note:\n",
        "# The following 'SamePadding' functions make output size equal ceil(input size/stride).\n",
        "# Only when stride equals 1, can the output size be the same as input size.\n",
        "# Don't be confused by their function names ! ! !\n",
        "\n",
        "def get_same_padding_conv2d(image_size=None):\n",
        "    \"\"\"Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n",
        "       Static padding is necessary for ONNX exporting of models.\n",
        "\n",
        "    Args:\n",
        "        image_size (int or tuple): Size of the image.\n",
        "\n",
        "    Returns:\n",
        "        Conv2dDynamicSamePadding or Conv2dStaticSamePadding.\n",
        "    \"\"\"\n",
        "    if image_size is None:\n",
        "        return Conv2dDynamicSamePadding\n",
        "    else:\n",
        "        return partial(Conv2dStaticSamePadding, image_size=image_size)\n",
        "\n",
        "\n",
        "class Conv2dDynamicSamePadding(nn.Conv2d):\n",
        "    \"\"\"2D Convolutions like TensorFlow, for a dynamic image size.\n",
        "       The padding is operated in forward function by calculating dynamically.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tips for 'SAME' mode padding.\n",
        "    #     Given the following:\n",
        "    #         i: width or height\n",
        "    #         s: stride\n",
        "    #         k: kernel size\n",
        "    #         d: dilation\n",
        "    #         p: padding\n",
        "    #     Output after Conv2d:\n",
        "    #         o = floor((i+p-((k-1)*d+1))/s+1)\n",
        "    # If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),\n",
        "    # => p = (i-1)*s+((k-1)*d+1)-i\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
        "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        ih, iw = x.size()[-2:]\n",
        "        kh, kw = self.weight.size()[-2:]\n",
        "        sh, sw = self.stride\n",
        "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)  # change the output size according to stride ! ! !\n",
        "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
        "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n",
        "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "class Conv2dStaticSamePadding(nn.Conv2d):\n",
        "    \"\"\"2D Convolutions like TensorFlow's 'SAME' mode, with the given input image size.\n",
        "       The padding mudule is calculated in construction function, then used in forward.\n",
        "    \"\"\"\n",
        "\n",
        "    # With the same calculation as Conv2dDynamicSamePadding\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, image_size=None, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, stride, **kwargs)\n",
        "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
        "\n",
        "        # Calculate padding based on image size and save it\n",
        "        assert image_size is not None\n",
        "        ih, iw = (image_size, image_size) if isinstance(image_size, int) else image_size\n",
        "        kh, kw = self.weight.size()[-2:]\n",
        "        sh, sw = self.stride\n",
        "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
        "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
        "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2,\n",
        "                                                pad_h // 2, pad_h - pad_h // 2))\n",
        "        else:\n",
        "            self.static_padding = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.static_padding(x)\n",
        "        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_same_padding_maxPool2d(image_size=None):\n",
        "    \"\"\"Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n",
        "       Static padding is necessary for ONNX exporting of models.\n",
        "\n",
        "    Args:\n",
        "        image_size (int or tuple): Size of the image.\n",
        "\n",
        "    Returns:\n",
        "        MaxPool2dDynamicSamePadding or MaxPool2dStaticSamePadding.\n",
        "    \"\"\"\n",
        "    if image_size is None:\n",
        "        return MaxPool2dDynamicSamePadding\n",
        "    else:\n",
        "        return partial(MaxPool2dStaticSamePadding, image_size=image_size)\n",
        "\n",
        "\n",
        "class MaxPool2dDynamicSamePadding(nn.MaxPool2d):\n",
        "    \"\"\"2D MaxPooling like TensorFlow's 'SAME' mode, with a dynamic image size.\n",
        "       The padding is operated in forward function by calculating dynamically.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size, stride, padding=0, dilation=1, return_indices=False, ceil_mode=False):\n",
        "        super().__init__(kernel_size, stride, padding, dilation, return_indices, ceil_mode)\n",
        "        self.stride = [self.stride] * 2 if isinstance(self.stride, int) else self.stride\n",
        "        self.kernel_size = [self.kernel_size] * 2 if isinstance(self.kernel_size, int) else self.kernel_size\n",
        "        self.dilation = [self.dilation] * 2 if isinstance(self.dilation, int) else self.dilation\n",
        "\n",
        "    def forward(self, x):\n",
        "        ih, iw = x.size()[-2:]\n",
        "        kh, kw = self.kernel_size\n",
        "        sh, sw = self.stride\n",
        "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
        "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
        "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n",
        "        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding,\n",
        "                            self.dilation, self.ceil_mode, self.return_indices)\n",
        "\n",
        "\n",
        "class MaxPool2dStaticSamePadding(nn.MaxPool2d):\n",
        "    \"\"\"2D MaxPooling like TensorFlow's 'SAME' mode, with the given input image size.\n",
        "       The padding mudule is calculated in construction function, then used in forward.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size, stride, image_size=None, **kwargs):\n",
        "        super().__init__(kernel_size, stride, **kwargs)\n",
        "        self.stride = [self.stride] * 2 if isinstance(self.stride, int) else self.stride\n",
        "        self.kernel_size = [self.kernel_size] * 2 if isinstance(self.kernel_size, int) else self.kernel_size\n",
        "        self.dilation = [self.dilation] * 2 if isinstance(self.dilation, int) else self.dilation\n",
        "\n",
        "        # Calculate padding based on image size and save it\n",
        "        assert image_size is not None\n",
        "        ih, iw = (image_size, image_size) if isinstance(image_size, int) else image_size\n",
        "        kh, kw = self.kernel_size\n",
        "        sh, sw = self.stride\n",
        "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
        "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
        "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
        "        else:\n",
        "            self.static_padding = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.static_padding(x)\n",
        "        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding,\n",
        "                         self.dilation, self.ceil_mode, self.return_indices)\n",
        "        return x\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Helper functions for loading model params\n",
        "################################################################################\n",
        "\n",
        "# BlockDecoder: A Class for encoding and decoding BlockArgs\n",
        "# efficientnet_params: A function to query compound coefficient\n",
        "# get_model_params and efficientnet:\n",
        "#     Functions to get BlockArgs and GlobalParams for efficientnet\n",
        "# url_map and url_map_advprop: Dicts of url_map for pretrained weights\n",
        "# load_pretrained_weights: A function to load pretrained weights\n",
        "\n",
        "class BlockDecoder(object):\n",
        "    \"\"\"Block Decoder for readability,\n",
        "       straight from the official TensorFlow repository.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _decode_block_string(block_string):\n",
        "        \"\"\"Get a block through a string notation of arguments.\n",
        "\n",
        "        Args:\n",
        "            block_string (str): A string notation of arguments.\n",
        "                                Examples: 'r1_k3_s11_e1_i32_o16_se0.25_noskip'.\n",
        "\n",
        "        Returns:\n",
        "            BlockArgs: The namedtuple defined at the top of this file.\n",
        "        \"\"\"\n",
        "        assert isinstance(block_string, str)\n",
        "\n",
        "        ops = block_string.split('_')\n",
        "        options = {}\n",
        "        for op in ops:\n",
        "            splits = re.split(r'(\\d.*)', op)\n",
        "            if len(splits) >= 2:\n",
        "                key, value = splits[:2]\n",
        "                options[key] = value\n",
        "\n",
        "        # Check stride\n",
        "        assert (('s' in options and len(options['s']) == 1) or\n",
        "                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n",
        "\n",
        "        return BlockArgs(\n",
        "            num_repeat=int(options['r']),\n",
        "            kernel_size=int(options['k']),\n",
        "            stride=[int(options['s'][0])],\n",
        "            expand_ratio=int(options['e']),\n",
        "            input_filters=int(options['i']),\n",
        "            output_filters=int(options['o']),\n",
        "            se_ratio=float(options['se']) if 'se' in options else None,\n",
        "            id_skip=('noskip' not in block_string))\n",
        "\n",
        "    @staticmethod\n",
        "    def _encode_block_string(block):\n",
        "        \"\"\"Encode a block to a string.\n",
        "\n",
        "        Args:\n",
        "            block (namedtuple): A BlockArgs type argument.\n",
        "\n",
        "        Returns:\n",
        "            block_string: A String form of BlockArgs.\n",
        "        \"\"\"\n",
        "        args = [\n",
        "            'r%d' % block.num_repeat,\n",
        "            'k%d' % block.kernel_size,\n",
        "            's%d%d' % (block.strides[0], block.strides[1]),\n",
        "            'e%s' % block.expand_ratio,\n",
        "            'i%d' % block.input_filters,\n",
        "            'o%d' % block.output_filters\n",
        "        ]\n",
        "        if 0 < block.se_ratio <= 1:\n",
        "            args.append('se%s' % block.se_ratio)\n",
        "        if block.id_skip is False:\n",
        "            args.append('noskip')\n",
        "        return '_'.join(args)\n",
        "\n",
        "    @staticmethod\n",
        "    def decode(string_list):\n",
        "        \"\"\"Decode a list of string notations to specify blocks inside the network.\n",
        "\n",
        "        Args:\n",
        "            string_list (list[str]): A list of strings, each string is a notation of block.\n",
        "\n",
        "        Returns:\n",
        "            blocks_args: A list of BlockArgs namedtuples of block args.\n",
        "        \"\"\"\n",
        "        assert isinstance(string_list, list)\n",
        "        blocks_args = []\n",
        "        for block_string in string_list:\n",
        "            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n",
        "        return blocks_args\n",
        "\n",
        "    @staticmethod\n",
        "    def encode(blocks_args):\n",
        "        \"\"\"Encode a list of BlockArgs to a list of strings.\n",
        "\n",
        "        Args:\n",
        "            blocks_args (list[namedtuples]): A list of BlockArgs namedtuples of block args.\n",
        "\n",
        "        Returns:\n",
        "            block_strings: A list of strings, each string is a notation of block.\n",
        "        \"\"\"\n",
        "        block_strings = []\n",
        "        for block in blocks_args:\n",
        "            block_strings.append(BlockDecoder._encode_block_string(block))\n",
        "        return block_strings\n",
        "\n",
        "\n",
        "def efficientnet_params(model_name):\n",
        "    \"\"\"Map EfficientNet model name to parameter coefficients.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Model name to be queried.\n",
        "\n",
        "    Returns:\n",
        "        params_dict[model_name]: A (width,depth,res,dropout) tuple.\n",
        "    \"\"\"\n",
        "    params_dict = {\n",
        "        # Coefficients:   width,depth,res,dropout\n",
        "        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
        "        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
        "        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
        "        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
        "        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
        "        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
        "        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
        "        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
        "        'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n",
        "        'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n",
        "    }\n",
        "    return params_dict[model_name]\n",
        "\n",
        "\n",
        "def efficientnet(width_coefficient=None, depth_coefficient=None, image_size=None,\n",
        "                 dropout_rate=0.2, drop_connect_rate=0.2, num_classes=1000, include_top=True):\n",
        "    \"\"\"Create BlockArgs and GlobalParams for efficientnet model.\n",
        "\n",
        "    Args:\n",
        "        width_coefficient (float)\n",
        "        depth_coefficient (float)\n",
        "        image_size (int)\n",
        "        dropout_rate (float)\n",
        "        drop_connect_rate (float)\n",
        "        num_classes (int)\n",
        "\n",
        "        Meaning as the name suggests.\n",
        "\n",
        "    Returns:\n",
        "        blocks_args, global_params.\n",
        "    \"\"\"\n",
        "\n",
        "    # Blocks args for the whole model(efficientnet-b0 by default)\n",
        "    # It will be modified in the construction of EfficientNet Class according to model\n",
        "    blocks_args = [\n",
        "        'r1_k3_s11_e1_i32_o16_se0.25',\n",
        "        'r2_k3_s22_e6_i16_o24_se0.25',\n",
        "        'r2_k5_s22_e6_i24_o40_se0.25',\n",
        "        'r3_k3_s22_e6_i40_o80_se0.25',\n",
        "        'r3_k5_s11_e6_i80_o112_se0.25',\n",
        "        'r4_k5_s22_e6_i112_o192_se0.25',\n",
        "        'r1_k3_s11_e6_i192_o320_se0.25',\n",
        "    ]\n",
        "    blocks_args = BlockDecoder.decode(blocks_args)\n",
        "\n",
        "    global_params = GlobalParams(\n",
        "        width_coefficient=width_coefficient,\n",
        "        depth_coefficient=depth_coefficient,\n",
        "        image_size=image_size,\n",
        "        dropout_rate=dropout_rate,\n",
        "\n",
        "        num_classes=num_classes,\n",
        "        batch_norm_momentum=0.99,\n",
        "        batch_norm_epsilon=1e-3,\n",
        "        drop_connect_rate=drop_connect_rate,\n",
        "        depth_divisor=8,\n",
        "        min_depth=None,\n",
        "        include_top=include_top,\n",
        "    )\n",
        "\n",
        "    return blocks_args, global_params\n",
        "\n",
        "\n",
        "def get_model_params(model_name, override_params):\n",
        "    \"\"\"Get the block args and global params for a given model name.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Model's name.\n",
        "        override_params (dict): A dict to modify global_params.\n",
        "\n",
        "    Returns:\n",
        "        blocks_args, global_params\n",
        "    \"\"\"\n",
        "    if model_name.startswith('efficientnet'):\n",
        "        w, d, s, p = efficientnet_params(model_name)\n",
        "        # note: all models have drop connect rate = 0.2\n",
        "        blocks_args, global_params = efficientnet(\n",
        "            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n",
        "    else:\n",
        "        raise NotImplementedError('model name is not pre-defined: {}'.format(model_name))\n",
        "    if override_params:\n",
        "        # ValueError will be raised here if override_params has fields not included in global_params.\n",
        "        global_params = global_params._replace(**override_params)\n",
        "    return blocks_args, global_params\n",
        "\n",
        "\n",
        "# train with Standard methods\n",
        "# check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks)\n",
        "url_map = {\n",
        "    'efficientnet-b0': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth',\n",
        "    'efficientnet-b1': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b1-f1951068.pth',\n",
        "    'efficientnet-b2': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth',\n",
        "    'efficientnet-b3': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth',\n",
        "    'efficientnet-b4': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth',\n",
        "    'efficientnet-b5': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b5-b6417697.pth',\n",
        "    'efficientnet-b6': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b6-c76e70fd.pth',\n",
        "    'efficientnet-b7': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth',\n",
        "}\n",
        "\n",
        "# train with Adversarial Examples(AdvProp)\n",
        "# check more details in paper(Adversarial Examples Improve Image Recognition)\n",
        "url_map_advprop = {\n",
        "    'efficientnet-b0': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b0-b64d5a18.pth',\n",
        "    'efficientnet-b1': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b1-0f3ce85a.pth',\n",
        "    'efficientnet-b2': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b2-6e9d97e5.pth',\n",
        "    'efficientnet-b3': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b3-cdd7c0f4.pth',\n",
        "    'efficientnet-b4': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b4-44fb3a87.pth',\n",
        "    'efficientnet-b5': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b5-86493f6b.pth',\n",
        "    'efficientnet-b6': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b6-ac80338e.pth',\n",
        "    'efficientnet-b7': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b7-4652b6dd.pth',\n",
        "    'efficientnet-b8': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b8-22a8fe65.pth',\n",
        "}\n",
        "\n",
        "# TODO: add the petrained weights url map of 'efficientnet-l2'\n",
        "\n",
        "\n",
        "def load_pretrained_weights(model, model_name, weights_path=None, load_fc=True, advprop=False, verbose=True):\n",
        "    \"\"\"Loads pretrained weights from weights path or download using url.\n",
        "\n",
        "    Args:\n",
        "        model (Module): The whole model of efficientnet.\n",
        "        model_name (str): Model name of efficientnet.\n",
        "        weights_path (None or str):\n",
        "            str: path to pretrained weights file on the local disk.\n",
        "            None: use pretrained weights downloaded from the Internet.\n",
        "        load_fc (bool): Whether to load pretrained weights for fc layer at the end of the model.\n",
        "        advprop (bool): Whether to load pretrained weights\n",
        "                        trained with advprop (valid when weights_path is None).\n",
        "    \"\"\"\n",
        "    if isinstance(weights_path, str):\n",
        "        state_dict = torch.load(weights_path)\n",
        "    else:\n",
        "        # AutoAugment or Advprop (different preprocessing)\n",
        "        url_map_ = url_map_advprop if advprop else url_map\n",
        "        state_dict = model_zoo.load_url(url_map_[model_name])\n",
        "\n",
        "    if load_fc:\n",
        "        ret = model.load_state_dict(state_dict, strict=False)\n",
        "        assert not ret.missing_keys, 'Missing keys when loading pretrained weights: {}'.format(ret.missing_keys)\n",
        "    else:\n",
        "        state_dict.pop('_fc.weight')\n",
        "        state_dict.pop('_fc.bias')\n",
        "        ret = model.load_state_dict(state_dict, strict=False)\n",
        "        assert set(ret.missing_keys) == set(\n",
        "            ['_fc.weight', '_fc.bias']), 'Missing keys when loading pretrained weights: {}'.format(ret.missing_keys)\n",
        "    assert not ret.unexpected_keys, 'Missing keys when loading pretrained weights: {}'.format(ret.unexpected_keys)\n",
        "\n",
        "    if verbose:\n",
        "        print('Loaded pretrained weights for {}'.format(model_name))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TPmReVlOXeUj"
      },
      "outputs": [],
      "source": [
        "\"\"\"model.py - Model and module class for EfficientNet.\n",
        "   They are built to mirror those in the official TensorFlow implementation.\n",
        "\"\"\"\n",
        "\n",
        "# Author: lukemelas (github username)\n",
        "# Github repo: https://github.com/lukemelas/EfficientNet-PyTorch\n",
        "# With adjustments and added comments by workingcoder (github username).\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "\n",
        "VALID_MODELS = (\n",
        "    'efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3',\n",
        "    'efficientnet-b4', 'efficientnet-b5', 'efficientnet-b6', 'efficientnet-b7',\n",
        "    'efficientnet-b8',\n",
        "\n",
        "    # Support the construction of 'efficientnet-l2' without pretrained weights\n",
        "    'efficientnet-l2'\n",
        ")\n",
        "\n",
        "\n",
        "class MBConvBlock(nn.Module):\n",
        "    \"\"\"Mobile Inverted Residual Bottleneck Block.\n",
        "\n",
        "    Args:\n",
        "        block_args (namedtuple): BlockArgs, defined in utils.py.\n",
        "        global_params (namedtuple): GlobalParam, defined in utils.py.\n",
        "        image_size (tuple or list): [image_height, image_width].\n",
        "\n",
        "    References:\n",
        "        [1] https://arxiv.org/abs/1704.04861 (MobileNet v1)\n",
        "        [2] https://arxiv.org/abs/1801.04381 (MobileNet v2)\n",
        "        [3] https://arxiv.org/abs/1905.02244 (MobileNet v3)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, block_args, global_params, image_size=None):\n",
        "        super().__init__()\n",
        "        self._block_args = block_args\n",
        "        self._bn_mom = 1 - global_params.batch_norm_momentum  # pytorch's difference from tensorflow\n",
        "        self._bn_eps = global_params.batch_norm_epsilon\n",
        "        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n",
        "        self.id_skip = block_args.id_skip  # whether to use skip connection and drop connect\n",
        "\n",
        "        # Expansion phase (Inverted Bottleneck)\n",
        "        inp = self._block_args.input_filters  # number of input channels\n",
        "        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n",
        "        if self._block_args.expand_ratio != 1:\n",
        "            Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
        "            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
        "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "            # image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size\n",
        "\n",
        "        # Depthwise convolution phase\n",
        "        k = self._block_args.kernel_size\n",
        "        s = self._block_args.stride\n",
        "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
        "        self._depthwise_conv = Conv2d(\n",
        "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
        "            kernel_size=k, stride=s, bias=False)\n",
        "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "        image_size = calculate_output_image_size(image_size, s)\n",
        "\n",
        "        # Squeeze and Excitation layer, if desired\n",
        "        if self.has_se:\n",
        "            Conv2d = get_same_padding_conv2d(image_size=(1, 1))\n",
        "            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n",
        "            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
        "            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
        "\n",
        "        # Pointwise convolution phase\n",
        "        final_oup = self._block_args.output_filters\n",
        "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
        "        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
        "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "        self._swish = MemoryEfficientSwish()\n",
        "\n",
        "    def forward(self, inputs, drop_connect_rate=None):\n",
        "        \"\"\"MBConvBlock's forward function.\n",
        "\n",
        "        Args:\n",
        "            inputs (tensor): Input tensor.\n",
        "            drop_connect_rate (bool): Drop connect rate (float, between 0 and 1).\n",
        "\n",
        "        Returns:\n",
        "            Output of this block after processing.\n",
        "        \"\"\"\n",
        "\n",
        "        # Expansion and Depthwise Convolution\n",
        "        x = inputs\n",
        "        if self._block_args.expand_ratio != 1:\n",
        "            x = self._expand_conv(inputs)\n",
        "            x = self._bn0(x)\n",
        "            x = self._swish(x)\n",
        "\n",
        "        x = self._depthwise_conv(x)\n",
        "        x = self._bn1(x)\n",
        "        x = self._swish(x)\n",
        "\n",
        "        # Squeeze and Excitation\n",
        "        if self.has_se:\n",
        "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
        "            x_squeezed = self._se_reduce(x_squeezed)\n",
        "            x_squeezed = self._swish(x_squeezed)\n",
        "            x_squeezed = self._se_expand(x_squeezed)\n",
        "            x = torch.sigmoid(x_squeezed) * x\n",
        "\n",
        "        # Pointwise Convolution\n",
        "        x = self._project_conv(x)\n",
        "        x = self._bn2(x)\n",
        "\n",
        "        # Skip connection and drop connect\n",
        "        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n",
        "        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n",
        "            # The combination of skip connection and drop connect brings about stochastic depth.\n",
        "            if drop_connect_rate:\n",
        "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
        "            x = x + inputs  # skip connection\n",
        "        return x\n",
        "\n",
        "    def set_swish(self, memory_efficient=True):\n",
        "        \"\"\"Sets swish function as memory efficient (for training) or standard (for export).\n",
        "\n",
        "        Args:\n",
        "            memory_efficient (bool): Whether to use memory-efficient version of swish.\n",
        "        \"\"\"\n",
        "        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    \"\"\"EfficientNet model.\n",
        "       Most easily loaded with the .from_name or .from_pretrained methods.\n",
        "\n",
        "    Args:\n",
        "        blocks_args (list[namedtuple]): A list of BlockArgs to construct blocks.\n",
        "        global_params (namedtuple): A set of GlobalParams shared between blocks.\n",
        "\n",
        "    References:\n",
        "        [1] https://arxiv.org/abs/1905.11946 (EfficientNet)\n",
        "\n",
        "    Example:\n",
        "        >>> import torch\n",
        "        >>> from efficientnet.model import EfficientNet\n",
        "        >>> inputs = torch.rand(1, 3, 224, 224)\n",
        "        >>> model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "        >>> model.eval()\n",
        "        >>> outputs = model(inputs)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, blocks_args=None, global_params=None):\n",
        "        super().__init__()\n",
        "        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
        "        assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
        "        self._global_params = global_params\n",
        "        self._blocks_args = blocks_args\n",
        "\n",
        "        # Batch norm parameters\n",
        "        bn_mom = 1 - self._global_params.batch_norm_momentum\n",
        "        bn_eps = self._global_params.batch_norm_epsilon\n",
        "\n",
        "        # Get stem static or dynamic convolution depending on image size\n",
        "        image_size = global_params.image_size\n",
        "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
        "\n",
        "        # Stem\n",
        "        in_channels = 3  # rgb\n",
        "        out_channels = round_filters(32, self._global_params)  # number of output channels\n",
        "        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
        "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
        "        image_size = calculate_output_image_size(image_size, 2)\n",
        "\n",
        "        # Build blocks\n",
        "        self._blocks = nn.ModuleList([])\n",
        "        for block_args in self._blocks_args:\n",
        "\n",
        "            # Update block input and output filters based on depth multiplier.\n",
        "            block_args = block_args._replace(\n",
        "                input_filters=round_filters(block_args.input_filters, self._global_params),\n",
        "                output_filters=round_filters(block_args.output_filters, self._global_params),\n",
        "                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n",
        "            )\n",
        "\n",
        "            # The first block needs to take care of stride and filter size increase.\n",
        "            self._blocks.append(MBConvBlock(block_args, self._global_params, image_size=image_size))\n",
        "            image_size = calculate_output_image_size(image_size, block_args.stride)\n",
        "            if block_args.num_repeat > 1:  # modify block_args to keep same output size\n",
        "                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n",
        "            for _ in range(block_args.num_repeat - 1):\n",
        "                self._blocks.append(MBConvBlock(block_args, self._global_params, image_size=image_size))\n",
        "                # image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1\n",
        "\n",
        "        # Head\n",
        "        in_channels = block_args.output_filters  # output of final block\n",
        "        out_channels = round_filters(1280, self._global_params)\n",
        "        Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
        "        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
        "\n",
        "        # Final linear layer\n",
        "        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "        if self._global_params.include_top:\n",
        "            self._dropout = nn.Dropout(self._global_params.dropout_rate)\n",
        "            self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n",
        "\n",
        "        # set activation to memory efficient swish by default\n",
        "        self._swish = MemoryEfficientSwish()\n",
        "\n",
        "    def set_swish(self, memory_efficient=True):\n",
        "        \"\"\"Sets swish function as memory efficient (for training) or standard (for export).\n",
        "\n",
        "        Args:\n",
        "            memory_efficient (bool): Whether to use memory-efficient version of swish.\n",
        "        \"\"\"\n",
        "        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n",
        "        for block in self._blocks:\n",
        "            block.set_swish(memory_efficient)\n",
        "\n",
        "    def extract_endpoints(self, inputs):\n",
        "        \"\"\"Use convolution layer to extract features\n",
        "        from reduction levels i in [1, 2, 3, 4, 5].\n",
        "\n",
        "        Args:\n",
        "            inputs (tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of last intermediate features\n",
        "            with reduction levels i in [1, 2, 3, 4, 5].\n",
        "            Example:\n",
        "                >>> import torch\n",
        "                >>> from efficientnet.model import EfficientNet\n",
        "                >>> inputs = torch.rand(1, 3, 224, 224)\n",
        "                >>> model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "                >>> endpoints = model.extract_endpoints(inputs)\n",
        "                >>> print(endpoints['reduction_1'].shape)  # torch.Size([1, 16, 112, 112])\n",
        "                >>> print(endpoints['reduction_2'].shape)  # torch.Size([1, 24, 56, 56])\n",
        "                >>> print(endpoints['reduction_3'].shape)  # torch.Size([1, 40, 28, 28])\n",
        "                >>> print(endpoints['reduction_4'].shape)  # torch.Size([1, 112, 14, 14])\n",
        "                >>> print(endpoints['reduction_5'].shape)  # torch.Size([1, 320, 7, 7])\n",
        "                >>> print(endpoints['reduction_6'].shape)  # torch.Size([1, 1280, 7, 7])\n",
        "        \"\"\"\n",
        "        endpoints = dict()\n",
        "\n",
        "        # Stem\n",
        "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
        "        prev_x = x\n",
        "\n",
        "        # Blocks\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)  # scale drop connect_rate\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "            if prev_x.size(2) > x.size(2):\n",
        "                endpoints['reduction_{}'.format(len(endpoints) + 1)] = prev_x\n",
        "            elif idx == len(self._blocks) - 1:\n",
        "                endpoints['reduction_{}'.format(len(endpoints) + 1)] = x\n",
        "            prev_x = x\n",
        "\n",
        "        # Head\n",
        "        x = self._swish(self._bn1(self._conv_head(x)))\n",
        "        endpoints['reduction_{}'.format(len(endpoints) + 1)] = x\n",
        "\n",
        "        return endpoints\n",
        "\n",
        "    def extract_features(self, inputs):\n",
        "        \"\"\"use convolution layer to extract feature .\n",
        "\n",
        "        Args:\n",
        "            inputs (tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Output of the final convolution\n",
        "            layer in the efficientnet model.\n",
        "        \"\"\"\n",
        "        # Stem\n",
        "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
        "\n",
        "        # Blocks\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)  # scale drop connect_rate\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "\n",
        "        # Head\n",
        "        x = self._swish(self._bn1(self._conv_head(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def delete_blocks(self, limit):\n",
        "        '''\n",
        "        tmp_blocks = nn.ModuleList([])\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            if idx < limit:\n",
        "                tmp_blocks.append(self._blocks)\n",
        "\n",
        "        self._blocks = tmp_blocks\n",
        "        '''\n",
        "        self._blocks = self._blocks\n",
        "\n",
        "    def extract_features_at_block(self, inputs, selected_block):\n",
        "        \"\"\"use convolution layer to extract feature .\n",
        "\n",
        "        Args:\n",
        "            inputs (tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Output of the final convolution\n",
        "            layer in the efficientnet model.\n",
        "        \"\"\"\n",
        "        # Stem\n",
        "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
        "\n",
        "        # Blocks\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)  # scale drop connect_rate\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "\n",
        "            if idx > selected_block:\n",
        "                break\n",
        "\n",
        "        # Head\n",
        "        if selected_block >= len(self._blocks):\n",
        "            x = self._swish(self._bn1(self._conv_head(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"EfficientNet's forward function.\n",
        "           Calls extract_features to extract features, applies final linear layer, and returns logits.\n",
        "\n",
        "        Args:\n",
        "            inputs (tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Output of this model after processing.\n",
        "        \"\"\"\n",
        "        # Convolution layers\n",
        "        x = self.extract_features(inputs)\n",
        "        # Pooling and final linear layer\n",
        "        x = self._avg_pooling(x)\n",
        "        if self._global_params.include_top:\n",
        "            x = x.flatten(start_dim=1)\n",
        "            x = self._dropout(x)\n",
        "            x = self._fc(x)\n",
        "        return x\n",
        "\n",
        "    @classmethod\n",
        "    def from_name(cls, model_name, in_channels=3, **override_params):\n",
        "        \"\"\"Create an efficientnet model according to name.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name for efficientnet.\n",
        "            in_channels (int): Input data's channel number.\n",
        "            override_params (other key word params):\n",
        "                Params to override model's global_params.\n",
        "                Optional key:\n",
        "                    'width_coefficient', 'depth_coefficient',\n",
        "                    'image_size', 'dropout_rate',\n",
        "                    'num_classes', 'batch_norm_momentum',\n",
        "                    'batch_norm_epsilon', 'drop_connect_rate',\n",
        "                    'depth_divisor', 'min_depth'\n",
        "\n",
        "        Returns:\n",
        "            An efficientnet model.\n",
        "        \"\"\"\n",
        "        cls._check_model_name_is_valid(model_name)\n",
        "        blocks_args, global_params = get_model_params(model_name, override_params)\n",
        "        model = cls(blocks_args, global_params)\n",
        "        model._change_in_channels(in_channels)\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_name, weights_path=None, advprop=False,\n",
        "                        in_channels=3, num_classes=1000, **override_params):\n",
        "        \"\"\"Create an efficientnet model according to name.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name for efficientnet.\n",
        "            weights_path (None or str):\n",
        "                str: path to pretrained weights file on the local disk.\n",
        "                None: use pretrained weights downloaded from the Internet.\n",
        "            advprop (bool):\n",
        "                Whether to load pretrained weights\n",
        "                trained with advprop (valid when weights_path is None).\n",
        "            in_channels (int): Input data's channel number.\n",
        "            num_classes (int):\n",
        "                Number of categories for classification.\n",
        "                It controls the output size for final linear layer.\n",
        "            override_params (other key word params):\n",
        "                Params to override model's global_params.\n",
        "                Optional key:\n",
        "                    'width_coefficient', 'depth_coefficient',\n",
        "                    'image_size', 'dropout_rate',\n",
        "                    'batch_norm_momentum',\n",
        "                    'batch_norm_epsilon', 'drop_connect_rate',\n",
        "                    'depth_divisor', 'min_depth'\n",
        "\n",
        "        Returns:\n",
        "            A pretrained efficientnet model.\n",
        "        \"\"\"\n",
        "        model = cls.from_name(model_name, num_classes=num_classes, **override_params)\n",
        "        load_pretrained_weights(model, model_name, weights_path=weights_path,\n",
        "                                load_fc=(num_classes == 1000), advprop=advprop)\n",
        "        model._change_in_channels(in_channels)\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def get_image_size(cls, model_name):\n",
        "        \"\"\"Get the input image size for a given efficientnet model.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name for efficientnet.\n",
        "\n",
        "        Returns:\n",
        "            Input image size (resolution).\n",
        "        \"\"\"\n",
        "        cls._check_model_name_is_valid(model_name)\n",
        "        _, _, res, _ = efficientnet_params(model_name)\n",
        "        return res\n",
        "\n",
        "    @classmethod\n",
        "    def _check_model_name_is_valid(cls, model_name):\n",
        "        \"\"\"Validates model name.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name for efficientnet.\n",
        "\n",
        "        Returns:\n",
        "            bool: Is a valid name or not.\n",
        "        \"\"\"\n",
        "        if model_name not in VALID_MODELS:\n",
        "            raise ValueError('model_name should be one of: ' + ', '.join(VALID_MODELS))\n",
        "\n",
        "    def _change_in_channels(self, in_channels):\n",
        "        \"\"\"Adjust model's first convolution layer to in_channels, if in_channels not equals 3.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Input data's channel number.\n",
        "        \"\"\"\n",
        "        if in_channels != 3:\n",
        "            Conv2d = get_same_padding_conv2d(image_size=self._global_params.image_size)\n",
        "            out_channels = round_filters(32, self._global_params)\n",
        "            self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pAL0aFYK5Bqf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# helpers\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "# pre-layernorm\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "# feedforward\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# attention\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context = None, kv_include_self = False):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        context = default(context, x)\n",
        "\n",
        "        if kv_include_self:\n",
        "            context = torch.cat((x, context), dim = 1) # cross attention requires CLS token includes itself as key / value\n",
        "\n",
        "        qkv = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "# transformer encoder, for small and large patches\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return self.norm(x)\n",
        "\n",
        "# projecting CLS tokens, in the case that small and large patch tokens have different dimensions\n",
        "\n",
        "class ProjectInOut(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "        need_projection = dim_in != dim_out\n",
        "        self.project_in = nn.Linear(dim_in, dim_out) if need_projection else nn.Identity()\n",
        "        self.project_out = nn.Linear(dim_out, dim_in) if need_projection else nn.Identity()\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        x = self.project_in(x)\n",
        "        x = self.fn(x, *args, **kwargs)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "# cross attention transformer\n",
        "\n",
        "class CrossTransformer(nn.Module):\n",
        "    def __init__(self, sm_dim, lg_dim, depth, heads, dim_head, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                ProjectInOut(sm_dim, lg_dim, PreNorm(lg_dim, Attention(lg_dim, heads = heads, dim_head = dim_head, dropout = dropout))),\n",
        "                ProjectInOut(lg_dim, sm_dim, PreNorm(sm_dim, Attention(sm_dim, heads = heads, dim_head = dim_head, dropout = dropout)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, sm_tokens, lg_tokens):\n",
        "        (sm_cls, sm_patch_tokens), (lg_cls, lg_patch_tokens) = map(lambda t: (t[:, :1], t[:, 1:]), (sm_tokens, lg_tokens))\n",
        "\n",
        "        for sm_attend_lg, lg_attend_sm in self.layers:\n",
        "            sm_cls = sm_attend_lg(sm_cls, context = lg_patch_tokens, kv_include_self = True) + sm_cls\n",
        "            lg_cls = lg_attend_sm(lg_cls, context = sm_patch_tokens, kv_include_self = True) + lg_cls\n",
        "\n",
        "        sm_tokens = torch.cat((sm_cls, sm_patch_tokens), dim = 1)\n",
        "        lg_tokens = torch.cat((lg_cls, lg_patch_tokens), dim = 1)\n",
        "        return sm_tokens, lg_tokens\n",
        "\n",
        "# multi-scale encoder\n",
        "\n",
        "class MultiScaleEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        depth,\n",
        "        sm_dim,\n",
        "        lg_dim,\n",
        "        sm_enc_params,\n",
        "        lg_enc_params,\n",
        "        cross_attn_heads,\n",
        "        cross_attn_depth,\n",
        "        cross_attn_dim_head = 64,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Transformer(dim = sm_dim, dropout = dropout, **sm_enc_params),\n",
        "                Transformer(dim = lg_dim, dropout = dropout, **lg_enc_params),\n",
        "                CrossTransformer(sm_dim = sm_dim, lg_dim = lg_dim, depth = cross_attn_depth, heads = cross_attn_heads, dim_head = cross_attn_dim_head, dropout = dropout)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, sm_tokens, lg_tokens):\n",
        "        for sm_enc, lg_enc, cross_attend in self.layers:\n",
        "            sm_tokens, lg_tokens = sm_enc(sm_tokens), lg_enc(lg_tokens)\n",
        "            sm_tokens, lg_tokens = cross_attend(sm_tokens, lg_tokens)\n",
        "\n",
        "        return sm_tokens, lg_tokens\n",
        "\n",
        "# patch-based image to token embedder\n",
        "\n",
        "class ImageEmbedder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        image_size,\n",
        "        patch_size,\n",
        "        dropout = 0.,\n",
        "        efficient_block = 8,\n",
        "        channels\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        self.efficient_net = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "        #self.efficient_net.delete_blocks(efficient_block)\n",
        "        self.efficient_block = efficient_block\n",
        "\n",
        "        for index, (name, param) in enumerate(self.efficient_net.named_parameters()):\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.efficient_net.extract_features_at_block(img, self.efficient_block)\n",
        "        '''\n",
        "        x_scaled = []\n",
        "        for idx, im in enumerate(x):\n",
        "            im = im.cpu().detach().numpy()\n",
        "            for patch_idx, patch in enumerate(im):\n",
        "                patch = 2.*(patch - np.min(patch))/np.ptp(patch)-1\n",
        "                im[patch_idx] = patch\n",
        "\n",
        "            x_scaled.append(im)\n",
        "        x = torch.tensor(x_scaled).cuda()\n",
        "        '''\n",
        "        #x = torch.tensor(x).cuda()\n",
        "        '''\n",
        "        for idx, im in enumerate(x):\n",
        "            im = im.cpu().detach().numpy()\n",
        "            for patch_idx, patch in enumerate(im):\n",
        "                cv2.imwrite(\"patches/patches_\"+str(idx)+\"_\"+str(patch_idx)+\".png\", patch)\n",
        "        '''\n",
        "        x = self.to_patch_embedding(x)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "# cross ViT class\n",
        "\n",
        "class CrossEfficientViT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config\n",
        "    ):\n",
        "        super().__init__()\n",
        "        image_size = config['model']['image-size']\n",
        "        num_classes = config['model']['num-classes']\n",
        "        sm_dim = config['model']['sm-dim']\n",
        "        sm_channels = config['model']['sm-channels']\n",
        "        lg_dim = config['model']['lg-dim']\n",
        "        lg_channels = config['model']['lg-channels']\n",
        "        sm_patch_size = config['model']['sm-patch-size']\n",
        "        sm_enc_depth = config['model']['sm-enc-depth']\n",
        "        sm_enc_heads = config['model']['sm-enc-heads']\n",
        "        sm_enc_mlp_dim = config['model']['sm-enc-mlp-dim']\n",
        "        sm_enc_dim_head = config['model']['sm-enc-dim-head']\n",
        "        lg_patch_size = config['model']['lg-patch-size']\n",
        "        lg_enc_depth = config['model']['lg-enc-depth']\n",
        "        lg_enc_mlp_dim = config['model']['lg-enc-mlp-dim']\n",
        "        lg_enc_heads = config['model']['lg-enc-heads']\n",
        "        lg_enc_dim_head = config['model']['lg-enc-dim-head']\n",
        "        cross_attn_depth = config['model']['cross-attn-depth']\n",
        "        cross_attn_heads = config['model']['cross-attn-heads']\n",
        "        cross_attn_dim_head = config['model']['cross-attn-dim-head']\n",
        "        depth = config['model']['depth']\n",
        "        dropout = config['model']['dropout']\n",
        "        emb_dropout = config['model']['emb-dropout']\n",
        "\n",
        "\n",
        "\n",
        "        self.sm_image_embedder = ImageEmbedder(dim = sm_dim, image_size = image_size, patch_size = sm_patch_size, dropout = emb_dropout, efficient_block = 16, channels=sm_channels)\n",
        "        self.lg_image_embedder = ImageEmbedder(dim = lg_dim, image_size = image_size, patch_size = lg_patch_size, dropout = emb_dropout, efficient_block = 1, channels=lg_channels)\n",
        "\n",
        "        self.multi_scale_encoder = MultiScaleEncoder(\n",
        "            depth = depth,\n",
        "            sm_dim = sm_dim,\n",
        "            lg_dim = lg_dim,\n",
        "            cross_attn_heads = cross_attn_heads,\n",
        "            cross_attn_dim_head = cross_attn_dim_head,\n",
        "            cross_attn_depth = cross_attn_depth,\n",
        "            sm_enc_params = dict(\n",
        "                depth = sm_enc_depth,\n",
        "                heads = sm_enc_heads,\n",
        "                mlp_dim = sm_enc_mlp_dim,\n",
        "                dim_head = sm_enc_dim_head\n",
        "            ),\n",
        "            lg_enc_params = dict(\n",
        "                depth = lg_enc_depth,\n",
        "                heads = lg_enc_heads,\n",
        "                mlp_dim = lg_enc_mlp_dim,\n",
        "                dim_head = lg_enc_dim_head\n",
        "            ),\n",
        "            dropout = dropout\n",
        "        )\n",
        "\n",
        "        self.sm_mlp_head = nn.Sequential(nn.LayerNorm(sm_dim), nn.Linear(sm_dim, num_classes))\n",
        "        self.lg_mlp_head = nn.Sequential(nn.LayerNorm(lg_dim), nn.Linear(lg_dim, num_classes))\n",
        "        self.output=nn.Sigmoid()\n",
        "\n",
        "    def forward(self, img):\n",
        "        sm_tokens = self.sm_image_embedder(img)\n",
        "        lg_tokens = self.lg_image_embedder(img)\n",
        "\n",
        "        sm_tokens, lg_tokens = self.multi_scale_encoder(sm_tokens, lg_tokens)\n",
        "\n",
        "        sm_cls, lg_cls = map(lambda t: t[:, 0], (sm_tokens, lg_tokens))\n",
        "\n",
        "        sm_logits = self.sm_mlp_head(sm_cls)\n",
        "        lg_logits = self.lg_mlp_head(lg_cls)\n",
        "        op=self.output(sm_logits + lg_logits)\n",
        "        return op"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "10b244c9162f4423b02481097852d35e",
            "d6f9baee066c426ab8eb9c5e8ee2701e",
            "c984440391c14eb283f2dd3ed757cf86",
            "fd5c482265b149bf8dfb11b391eaf835",
            "d669b1bb322a4d7794e1b65769fa975b",
            "ac0d4a8333554b53b60dd1f8f289803f",
            "36bf6ac2bf234a6b8c264155ea0a5b6c",
            "52757dae9b1d47e48e9728d11f0fcaf8",
            "d6b79482b3db4ddd807db138a3ef95fd",
            "f9041ff714df4c2593f3c7a0eaba0254",
            "adb5f29b4fe8490c8cfb7f9f5a3ae1dc"
          ]
        },
        "id": "Mo_bcuNSVX5b",
        "outputId": "417116cb-af07-4e07-96a0-d1d35e613ad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = CrossEfficientViT(config).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qf5HjwVmmVE3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x_real' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m x_train\u001b[39m=\u001b[39m[]\n\u001b[0;32m      2\u001b[0m y_train\u001b[39m=\u001b[39m[]\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(x_real)):\n\u001b[0;32m      4\u001b[0m   x_train\u001b[39m.\u001b[39mappend(x_real[i])\n\u001b[0;32m      5\u001b[0m   y_train\u001b[39m.\u001b[39mappend(y_real[i])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'x_real' is not defined"
          ]
        }
      ],
      "source": [
        "x_train=[]\n",
        "y_train=[]\n",
        "for i in range(0,len(x_real)):\n",
        "  x_train.append(x_real[i])\n",
        "  y_train.append(y_real[i])\n",
        "for i in range(0,len(x_fake)):\n",
        "  x_train.append(x_fake[i])\n",
        "  y_train.append(y_fake[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Ni9AIDHRmeLx"
      },
      "outputs": [],
      "source": [
        "x_test=[]\n",
        "y_test=[]\n",
        "for i in range(0,len(x_real_val)):\n",
        "  x_test.append(x_real_val[i])\n",
        "  y_test.append(y_real_val[i])\n",
        "for i in range(0,len(x_fake_val)):\n",
        "  x_test.append(x_fake_val[i])\n",
        "  y_test.append(y_fake_val[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "83nhqCcsmrWE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class customDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform = None):\n",
        "        self.data = data\n",
        "        self.labels = torch.FloatTensor(labels)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        if len(self.labels):\n",
        "          y = self.labels[index]\n",
        "        else:\n",
        "          y = None\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "yOsqeUzHmt9P"
      },
      "outputs": [],
      "source": [
        "train_dataset = customDataset(x_train,y_train)\n",
        "test_dataset=customDataset(x_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "u9pj17CqmwZw"
      },
      "outputs": [],
      "source": [
        "train_loader=DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=32,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "QTNHoWtwm08f"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "N0WP7x7a_VMQ"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "lr = 0.01\n",
        "optimizer = optim.SGD\n",
        "criterion = torch.nn.functional.binary_cross_entropy_with_logits\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def accuracy(y_pred, y):\n",
        "   # predicted = torch.max(y_pred.data)\n",
        "    predicted=torch.round(y_pred)\n",
        "    total = y.size(0)\n",
        "    correct = (predicted == y).sum().item()\n",
        "    return correct/total\n",
        "\n",
        "def train(model, dataset, opt_fn, criterion,epoch,learning_rate):\n",
        "\n",
        "    optimizer=opt_fn(model.parameters(),learning_rate)\n",
        "    model.train()\n",
        "    train_loss=[]\n",
        "    train_acc=[]\n",
        "    for batch_idx,(data,target) in enumerate(dataset):\n",
        "        optimizer.zero_grad()\n",
        "        output=model.forward(data)\n",
        "        output=torch.squeeze(output,1)\n",
        "        loss=criterion(output,target)\n",
        "        bceloss=torch.nn.BCELoss()\n",
        "        logloss=bceloss(output,target)\n",
        "        logloss.backward()\n",
        "        optimizer.step()\n",
        "        acc = accuracy(output, target)\n",
        "        train_acc.append(acc)\n",
        "        train_loss.append(logloss.item())\n",
        "        print('\\repoch:{}({:.0f}%)\\tloss:{:.3f}\\ttrain_accuracy:{:.2f}'.format(epoch+1,100*batch_idx/len(dataset),\n",
        "        np.mean(train_loss),100*np.mean(train_acc)),end='')\n",
        "\n",
        "def eval(model, dataset, criterion):\n",
        "\n",
        "    model.eval()\n",
        "    val_acc=[]\n",
        "    for batch_idx,(data,target) in enumerate(dataset):\n",
        "        output=model.forward(data)\n",
        "        output=output.squeeze(1)\n",
        "        acc = accuracy(output, target)\n",
        "        val_acc.append(acc)\n",
        "    print('val_accuracy:{:.2f}%'.format(100*np.mean(val_acc)))\n",
        "    return np.mean(val_acc)\n",
        "\n",
        "def evaluate(model, dataset, criterion):\n",
        "\n",
        "    model.eval()\n",
        "    output_final = 0\n",
        "    for batch_idx,(data) in enumerate(dataset):\n",
        "        output=model.forward(data)\n",
        "        print(output)\n",
        "        output_final += output.squeeze(1)\n",
        "    y_final = torch.round(torch.mean(output_final))\n",
        "    print(y_final)\n",
        "    if y_final == 0:\n",
        "        return 'fake'\n",
        "    else:\n",
        "        return 'real'\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iDLTLWr_Y4Z",
        "outputId": "06a7f7d0-9e92-463f-b3de-90bd6311dc0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:1(100%)\tloss:0.684\ttrain_accuracy:58.36%val_accuracy:57.64%\n",
            "TIME TAKEN FOR THE EPOCH: 53 mins and 6 seconds\n",
            "\n",
            "epoch:2(100%)\tloss:0.471\ttrain_accuracy:77.08%val_accuracy:81.78%\n",
            "TIME TAKEN FOR THE EPOCH: 78 mins and 19 seconds\n",
            "\n",
            "epoch:3(100%)\tloss:0.325\ttrain_accuracy:85.43%val_accuracy:86.29%\n",
            "TIME TAKEN FOR THE EPOCH: 120 mins and 8 seconds\n",
            "\n",
            "epoch:4(100%)\tloss:0.236\ttrain_accuracy:89.60%val_accuracy:88.84%\n",
            "TIME TAKEN FOR THE EPOCH: 129 mins and 27 seconds\n",
            "\n",
            "epoch:5(100%)\tloss:0.183\ttrain_accuracy:92.17%val_accuracy:91.34%\n",
            "TIME TAKEN FOR THE EPOCH: 78 mins and 12 seconds\n",
            "\n",
            "epoch:6(100%)\tloss:0.143\ttrain_accuracy:93.85%val_accuracy:90.35%\n",
            "TIME TAKEN FOR THE EPOCH: 68 mins and 58 seconds\n",
            "\n",
            "epoch:7(100%)\tloss:0.114\ttrain_accuracy:95.10%val_accuracy:92.49%\n",
            "TIME TAKEN FOR THE EPOCH: 70 mins and 10 seconds\n",
            "\n",
            "epoch:8(100%)\tloss:0.102\ttrain_accuracy:95.72%val_accuracy:92.78%\n",
            "TIME TAKEN FOR THE EPOCH: 52 mins and 48 seconds\n",
            "\n",
            "epoch:9(100%)\tloss:0.082\ttrain_accuracy:96.62%val_accuracy:94.28%\n",
            "TIME TAKEN FOR THE EPOCH: 52 mins and 49 seconds\n",
            "\n",
            "epoch:10(100%)\tloss:0.072\ttrain_accuracy:97.10%val_accuracy:91.78%\n",
            "TIME TAKEN FOR THE EPOCH: 52 mins and 51 seconds\n",
            "\n",
            "epoch:11(100%)\tloss:0.060\ttrain_accuracy:97.69%val_accuracy:94.48%\n",
            "TIME TAKEN FOR THE EPOCH: 154 mins and 9 seconds\n",
            "\n",
            "epoch:12(100%)\tloss:0.056\ttrain_accuracy:97.77%val_accuracy:95.41%\n",
            "TIME TAKEN FOR THE EPOCH: 56 mins and 27 seconds\n",
            "\n",
            "epoch:13(95%)\tloss:0.051\ttrain_accuracy:98.02%"
          ]
        }
      ],
      "source": [
        "checkpoint_interval = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "      start_time = time.monotonic()\n",
        "      train(model, train_loader, optimizer, criterion,epoch,lr)\n",
        "      eval_accuracy = eval(model, test_loader, criterion)\n",
        "      end_time = time.monotonic()\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "      print(\"TIME TAKEN FOR THE EPOCH: {} mins and {} seconds\\n\".format(epoch_mins, epoch_secs))\n",
        "      # Save checkpoint at specified interval\n",
        "      if epoch % checkpoint_interval == 0:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer(model.parameters(), lr).state_dict(),\n",
        "            'loss': criterion,  # Save the loss function state if necessary\n",
        "            'accuracy': eval_accuracy,  # Save any other information you need\n",
        "          }\n",
        "        torch.save(checkpoint, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "\n",
        "# Save the final model after training is complete\n",
        "torch.save(model.state_dict(), \"final_model_weights.pth\")\n",
        "print(\"OVERALL TRAINING COMPLETE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:13(100%)\tloss:0.084\ttrain_accuracy:96.77val_accuracy:97.76%\n",
            "TIME TAKEN FOR THE EPOCH: 54 mins and 1 seconds\n",
            "\n",
            "epoch:14(100%)\tloss:0.064\ttrain_accuracy:97.51val_accuracy:98.45%\n",
            "TIME TAKEN FOR THE EPOCH: 71 mins and 48 seconds\n",
            "\n",
            "epoch:15(100%)\tloss:0.050\ttrain_accuracy:98.14val_accuracy:98.11%\n",
            "TIME TAKEN FOR THE EPOCH: 51 mins and 20 seconds\n",
            "\n",
            "epoch:16(100%)\tloss:0.042\ttrain_accuracy:98.40val_accuracy:98.54%\n",
            "TIME TAKEN FOR THE EPOCH: 50 mins and 46 seconds\n",
            "\n",
            "epoch:17(100%)\tloss:0.041\ttrain_accuracy:98.43val_accuracy:98.57%\n",
            "TIME TAKEN FOR THE EPOCH: 49 mins and 24 seconds\n",
            "\n",
            "epoch:18(100%)\tloss:0.034\ttrain_accuracy:98.77val_accuracy:98.44%\n",
            "TIME TAKEN FOR THE EPOCH: 49 mins and 31 seconds\n",
            "\n",
            "epoch:19(100%)\tloss:0.030\ttrain_accuracy:98.88val_accuracy:98.54%\n",
            "TIME TAKEN FOR THE EPOCH: 49 mins and 29 seconds\n",
            "\n",
            "epoch:20(100%)\tloss:0.027\ttrain_accuracy:99.04val_accuracy:96.37%\n",
            "TIME TAKEN FOR THE EPOCH: 49 mins and 31 seconds\n",
            "\n",
            "OVERALL TRAINING COMPLETE\n"
          ]
        }
      ],
      "source": [
        "# Set the starting epoch (0 if starting from scratch, or the last \n",
        "# Check if a checkpoint exists and load it\n",
        "\n",
        "start_epoch = 11\n",
        "\n",
        "checkpoint_interval = 1\n",
        "\n",
        "checkpoint_file = \"checkpoint_epoch_{}.pth\".format(start_epoch)\n",
        "if torch.cuda.is_available():\n",
        "    checkpoint = torch.load(checkpoint_file)\n",
        "else:\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=torch.device('cpu'))\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(optimizer(model.parameters(), lr), checkpoint['optimizer_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "# Load any other relevant information from the checkpoint\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "      start_time = time.monotonic()\n",
        "      train(model, train_loader, optimizer, criterion,epoch,lr)\n",
        "      eval_accuracy = eval(model, test_loader, criterion)\n",
        "      end_time = time.monotonic()\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "      print(\"TIME TAKEN FOR THE EPOCH: {} mins and {} seconds\\n\".format(epoch_mins, epoch_secs))\n",
        "      # Save checkpoint at specified interval\n",
        "      if epoch % checkpoint_interval == 0:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer(model.parameters(), lr).state_dict(),\n",
        "            'loss': criterion,  # Save the loss function state if necessary\n",
        "            'accuracy': eval_accuracy,  # Save any other information you need\n",
        "          }\n",
        "        torch.save(checkpoint, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "\n",
        "# Save the final model after training is complete\n",
        "torch.save(model.state_dict(), \"final_model_weights.pth\")\n",
        "print(\"OVERALL TRAINING COMPLETE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"final_model_weights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOX5W9XngvxA"
      },
      "outputs": [],
      "source": [
        "eval(model, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ],
      "source": [
        "model = CrossEfficientViT(config).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"checkpoint_epoch_17.pth\", map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(optimizer(model.parameters(), lr), checkpoint['optimizer_state_dict'])\n",
        "torch.save(model.state_dict(), \"final_model_weights.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for CrossEfficientViT(\n",
            "  (sm_image_embedder): ImageEmbedder(\n",
            "    (efficient_net): EfficientNet(\n",
            "      (_conv_stem): Conv2dStaticSamePadding(\n",
            "        3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
            "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
            "      )\n",
            "      (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "      (_blocks): ModuleList(\n",
            "        (0): MBConvBlock(\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (1): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
            "            (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (2): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (3): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (4): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
            "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (5): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
            "            (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (6-7): 2 x MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (8): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
            "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (9-10): 2 x MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
            "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (11): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (12-14): 3 x MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
            "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (15): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "      )\n",
            "      (_conv_head): Conv2dStaticSamePadding(\n",
            "        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (static_padding): Identity()\n",
            "      )\n",
            "      (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "      (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
            "      (_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (_fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
            "      (_swish): MemoryEfficientSwish()\n",
            "    )\n",
            "    (to_patch_embedding): Sequential(\n",
            "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=7, p2=7)\n",
            "      (1): Linear(in_features=62720, out_features=192, bias=True)\n",
            "    )\n",
            "    (dropout): Dropout(p=0.15, inplace=False)\n",
            "  )\n",
            "  (lg_image_embedder): ImageEmbedder(\n",
            "    (efficient_net): EfficientNet(\n",
            "      (_conv_stem): Conv2dStaticSamePadding(\n",
            "        3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
            "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
            "      )\n",
            "      (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "      (_blocks): ModuleList(\n",
            "        (0): MBConvBlock(\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (1): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
            "            (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (2): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (3): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (4): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
            "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (5): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
            "            (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (6-7): 2 x MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (8): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
            "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (9-10): 2 x MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
            "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (11): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (12-14): 3 x MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
            "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "        (15): MBConvBlock(\n",
            "          (_expand_conv): Conv2dStaticSamePadding(\n",
            "            192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
            "            1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
            "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
            "          )\n",
            "          (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_se_reduce): Conv2dStaticSamePadding(\n",
            "            1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_se_expand): Conv2dStaticSamePadding(\n",
            "            48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_project_conv): Conv2dStaticSamePadding(\n",
            "            1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (static_padding): Identity()\n",
            "          )\n",
            "          (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "          (_swish): MemoryEfficientSwish()\n",
            "        )\n",
            "      )\n",
            "      (_conv_head): Conv2dStaticSamePadding(\n",
            "        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (static_padding): Identity()\n",
            "      )\n",
            "      (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
            "      (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
            "      (_dropout): Dropout(p=0.2, inplace=False)\n",
            "      (_fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
            "      (_swish): MemoryEfficientSwish()\n",
            "    )\n",
            "    (to_patch_embedding): Sequential(\n",
            "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=56, p2=56)\n",
            "      (1): Linear(in_features=75264, out_features=384, bias=True)\n",
            "    )\n",
            "    (dropout): Dropout(p=0.15, inplace=False)\n",
            "  )\n",
            "  (multi_scale_encoder): MultiScaleEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x ModuleList(\n",
            "        (0): Transformer(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x ModuleList(\n",
            "              (0): PreNorm(\n",
            "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "                (fn): Attention(\n",
            "                  (attend): Softmax(dim=-1)\n",
            "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
            "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
            "                  (to_out): Sequential(\n",
            "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
            "                    (1): Dropout(p=0.15, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (1): PreNorm(\n",
            "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "                (fn): FeedForward(\n",
            "                  (net): Sequential(\n",
            "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
            "                    (1): GELU(approximate='none')\n",
            "                    (2): Dropout(p=0.15, inplace=False)\n",
            "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
            "                    (4): Dropout(p=0.15, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): Transformer(\n",
            "          (layers): ModuleList(\n",
            "            (0-2): 3 x ModuleList(\n",
            "              (0): PreNorm(\n",
            "                (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "                (fn): Attention(\n",
            "                  (attend): Softmax(dim=-1)\n",
            "                  (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
            "                  (to_kv): Linear(in_features=384, out_features=1024, bias=False)\n",
            "                  (to_out): Sequential(\n",
            "                    (0): Linear(in_features=512, out_features=384, bias=True)\n",
            "                    (1): Dropout(p=0.15, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (1): PreNorm(\n",
            "                (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "                (fn): FeedForward(\n",
            "                  (net): Sequential(\n",
            "                    (0): Linear(in_features=384, out_features=2048, bias=True)\n",
            "                    (1): GELU(approximate='none')\n",
            "                    (2): Dropout(p=0.15, inplace=False)\n",
            "                    (3): Linear(in_features=2048, out_features=384, bias=True)\n",
            "                    (4): Dropout(p=0.15, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): CrossTransformer(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x ModuleList(\n",
            "              (0): ProjectInOut(\n",
            "                (fn): PreNorm(\n",
            "                  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "                  (fn): Attention(\n",
            "                    (attend): Softmax(dim=-1)\n",
            "                    (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
            "                    (to_kv): Linear(in_features=384, out_features=1024, bias=False)\n",
            "                    (to_out): Sequential(\n",
            "                      (0): Linear(in_features=512, out_features=384, bias=True)\n",
            "                      (1): Dropout(p=0.15, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (project_in): Linear(in_features=192, out_features=384, bias=True)\n",
            "                (project_out): Linear(in_features=384, out_features=192, bias=True)\n",
            "              )\n",
            "              (1): ProjectInOut(\n",
            "                (fn): PreNorm(\n",
            "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "                  (fn): Attention(\n",
            "                    (attend): Softmax(dim=-1)\n",
            "                    (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
            "                    (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
            "                    (to_out): Sequential(\n",
            "                      (0): Linear(in_features=512, out_features=192, bias=True)\n",
            "                      (1): Dropout(p=0.15, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                )\n",
            "                (project_in): Linear(in_features=384, out_features=192, bias=True)\n",
            "                (project_out): Linear(in_features=192, out_features=384, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sm_mlp_head): Sequential(\n",
            "    (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=192, out_features=1, bias=True)\n",
            "  )\n",
            "  (lg_mlp_head): Sequential(\n",
            "    (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=384, out_features=1, bias=True)\n",
            "  )\n",
            "  (output): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "load_pretrained_weights(model=model, model_name=model, weights_path=\"final_model_weights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob as gb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "CLASS_INDEX = None\n",
        "CLASS_INDEX_PATH = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_input(x, dim_ordering='default'):\n",
        "    if dim_ordering == 'default':\n",
        "        dim_ordering = K.image_data_format()\n",
        "    x = x.astype(float)\n",
        "    if dim_ordering == 'th':\n",
        "        x[:, 0, :, :] -= 103.939\n",
        "        x[:, 1, :, :] -= 116.779\n",
        "        x[:, 2, :, :] -= 123.68\n",
        "        # 'RGB'->'BGR'\n",
        "        x = x[:, ::-1, :, :]\n",
        "    else:\n",
        "        x[:, :, :, 0] -= 103.939\n",
        "        x[:, :, :, 1] -= 116.779\n",
        "        x[:, :, :, 2] -= 123.68\n",
        "        # 'RGB'->'BGR'\n",
        "        x = x[:, :, :, ::-1]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_predictions(preds, top=5):\n",
        "    global CLASS_INDEX\n",
        "    if len(preds.shape) != 2 or preds.shape[1] != 512:\n",
        "        raise ValueError('`decode_predictions` expects '\n",
        "                         'a batch of predictions '\n",
        "                         '(i.e. a 2D array of shape (samples, 1000)). '\n",
        "                         'Found array with shape: ' + str(preds.shape))\n",
        "    if CLASS_INDEX is None:\n",
        "        fpath = get_file('imagenet_class_index.json',\n",
        "                         CLASS_INDEX_PATH,\n",
        "                         cache_subdir='models')\n",
        "        CLASS_INDEX = json.load(open(fpath))\n",
        "    results = []\n",
        "    for pred in preds:\n",
        "        top_indices = pred.argsort()[-top:][::-1]\n",
        "        result = [tuple(CLASS_INDEX[str(i)]) + (pred[i],) for i in top_indices]\n",
        "        results.append(result)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "from keras import backend\n",
        "from keras.applications import imagenet_utils\n",
        "from keras.engine import training\n",
        "from keras.layers import VersionAwareLayers\n",
        "from keras.utils import data_utils\n",
        "from keras.utils import layer_utils\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
        "\n",
        "# isort: off\n",
        "from tensorflow.python.util.tf_export import keras_export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def VGG16(\n",
        "    include_top=True,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    input_shape=None,\n",
        "    pooling=None,\n",
        "    classes=1000,\n",
        "    classifier_activation=\"softmax\",\n",
        "):\n",
        "    if not (weights in {\"imagenet\", None} or tf.io.gfile.exists(weights)):\n",
        "        raise ValueError(\n",
        "            \"The `weights` argument should be either \"\n",
        "            \"`None` (random initialization), `imagenet` \"\n",
        "            \"(pre-training on ImageNet), \"\n",
        "            \"or the path to the weights file to be loaded.  Received: \"\n",
        "            f\"weights={weights}\"\n",
        "        )\n",
        "\n",
        "    if weights == \"imagenet\" and include_top and classes != 1000:\n",
        "        raise ValueError(\n",
        "            'If using `weights` as `\"imagenet\"` with `include_top` '\n",
        "            \"as true, `classes` should be 1000.  \"\n",
        "            f\"Received `classes={classes}`\"\n",
        "        )\n",
        "    # Determine proper input shape\n",
        "    input_shape = imagenet_utils.obtain_input_shape(\n",
        "        input_shape,\n",
        "        default_size=224,\n",
        "        min_size=32,\n",
        "        data_format=backend.image_data_format(),\n",
        "        require_flatten=include_top,\n",
        "        weights=weights,\n",
        "    )\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not backend.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "    # Block 1\n",
        "    x = Conv2D(\n",
        "        64, (3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv1\"\n",
        "    )(img_input)\n",
        "    x = Conv2D(\n",
        "        64, (3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv2\"\n",
        "    )(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block1_pool\")(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(\n",
        "        128, (3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv1\"\n",
        "    )(x)\n",
        "    x = Conv2D(\n",
        "        128, (3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv2\"\n",
        "    )(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block2_pool\")(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(\n",
        "        256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv1\"\n",
        "    )(x)\n",
        "    x = Conv2D(\n",
        "        256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv2\"\n",
        "    )(x)\n",
        "    x = Conv2D(\n",
        "        256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv3\"\n",
        "    )(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block3_pool\")(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(\n",
        "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv1\"\n",
        "    )(x)\n",
        "    x = Conv2D(\n",
        "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv2\"\n",
        "    )(x)\n",
        "    x = Conv2D(\n",
        "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv3\"\n",
        "    )(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block4_pool\")(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(\n",
        "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv1\"\n",
        "    )(x)\n",
        "    x = Conv2D(\n",
        "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv2\"\n",
        "    )(x)\n",
        "    x = Conv2D(\n",
        "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv3\"\n",
        "    )(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block5_pool\")(x)\n",
        "\n",
        "    if include_top:\n",
        "        # Classification block\n",
        "        x = Flatten(name=\"flatten\")(x)\n",
        "        x = Dense(4096, activation=\"relu\", name=\"fc1\")(x)\n",
        "        x = Dense(4096, activation=\"relu\", name=\"fc2\")(x)\n",
        "\n",
        "        imagenet_utils.validate_activation(classifier_activation, weights)\n",
        "        x = Dense(\n",
        "            classes, activation=classifier_activation, name=\"predictions\"\n",
        "        )(x)\n",
        "    else:\n",
        "        if pooling == \"avg\":\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "        elif pooling == \"max\":\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = layer_utils.get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "    # Create model.\n",
        "    model = training.Model(inputs, x, name=\"vgg16\")\n",
        "\n",
        "    # Load weights.\n",
        "    if weights == \"imagenet\":\n",
        "        if include_top:\n",
        "            weights_path = data_utils.get_file(\n",
        "                \"vgg16_weights_tf_dim_ordering_tf_kernels.h5\",\n",
        "                TF_WEIGHTS_PATH,\n",
        "                cache_subdir=\"models\",\n",
        "                file_hash=\"64373286793e3c8b2b4e3219cbf3544b\",\n",
        "            )\n",
        "        else:\n",
        "            weights_path = data_utils.get_file(\n",
        "                \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n",
        "                TF_WEIGHTS_PATH_NO_TOP,\n",
        "                cache_subdir=\"models\",\n",
        "                file_hash=\"6d6bbae143d832006294945121d1f1fc\",\n",
        "            )\n",
        "        model.load_weights(weights_path)\n",
        "    elif weights is not None:\n",
        "        model.load_weights(weights)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = VGG16(weights='imagenet',include_top=False, pooling='avg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 359ms/step\n",
            "(1, 512)\n",
            "Predicted: Afghan_hound\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    img_path = 'elephant.jpg'\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize((224, 224))\n",
        "    x = np.array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    print('Input image shape:', x.shape)\n",
        "\n",
        "    preds = model.predict(x)\n",
        "    print(preds.shape)\n",
        "    pred = decode_predictions(preds)\n",
        "    print('Predicted:', pred[0][0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cnn_feat(frames_raw, frames_shape):\n",
        "    frames=[]\n",
        "    pca = PCA(n_components=frames_shape)\n",
        "    for im in frames_raw:\n",
        "        #print im.shape\n",
        "        im = cv2.resize(im, (224, 224)).astype(np.float32)\n",
        "        im[:,:,0] -= 103.939\n",
        "        im[:,:,1] -= 116.779\n",
        "        im[:,:,2] -= 123.68\n",
        "        # print im.shape\n",
        "        im = np.expand_dims(im, axis=0)\n",
        "        #print im.shape\n",
        "        frames.append(np.asarray(im))\n",
        "    frames = np.array(frames)\n",
        "    #print frames.shape\n",
        "\n",
        "    model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "    i = 0\n",
        "    features = np.ndarray((frames.shape[0], 512), dtype=np.float32)\n",
        "    for x in frames:\n",
        "        print(model.predict(x).shape)\n",
        "        #print x.shape\n",
        "        features[i,:] = model.predict(x)\n",
        "        i+=1\n",
        "    return pca.fit_transform(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_color_hist(frames_raw, num_bins):\n",
        "    print (\"Generating linear Histrograms using OpenCV\")\n",
        "    channels=['b','g','r']\n",
        "    \n",
        "    hist=[]\n",
        "    for frame in frames_raw:\n",
        "        feature_value=[cv2.calcHist([frame],[i],None,[int(num_bins)],[0,256]) for i,col in enumerate(channels)]\n",
        "        hist.append(np.asarray(feature_value).flatten())\n",
        "    \n",
        "    hist=np.asarray(hist)\n",
        "    #print \"Done generating!\"\n",
        "    print (\"Shape of histogram: \" + str(hist.shape))\n",
        "    \n",
        "    return hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generic VSUMM to test with different features\n",
        "# k means clustering to generate video summary\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import scipy.io\n",
        "import os\n",
        "\n",
        "# k-means\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import IPython.display \n",
        "from IPython.display import Video, Image, display\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# frame chosen every k frames\n",
        "sampling_rate= 7\n",
        "\n",
        "# percent of video for summary\n",
        "percent= 3\n",
        "\n",
        "# globalizing\n",
        "num_centroids=0\n",
        "SaveFrames = False\n",
        "SaveKeyFrames = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Opening video!\n",
            "Video opened\n",
            "Choosing frames\n",
            "Frames chosen\n",
            "Length of video 64\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "(1, 512)\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "Shape of features (64, 64)\n",
            "Clustering\n",
            "Done Clustering!\n",
            "Generating summary frames\n",
            "[5, 9, 12, 17, 19, 21, 25, 27, 31, 44, 50, 58, 62]\n",
            "Generated summary\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sanje\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2369],\n",
            "        [0.2293],\n",
            "        [0.2205],\n",
            "        [0.3086],\n",
            "        [0.2360],\n",
            "        [0.2621],\n",
            "        [0.2910],\n",
            "        [0.2692],\n",
            "        [0.2434],\n",
            "        [0.2299],\n",
            "        [0.3616],\n",
            "        [0.2167],\n",
            "        [0.2876]], grad_fn=<SigmoidBackward0>)\n",
            "tensor(0., grad_fn=<RoundBackward0>)\n",
            "fake\n"
          ]
        }
      ],
      "source": [
        "def main(model, criterion):\n",
        "    global num_bins, sampling_rate, num_centroids, percent\n",
        "    print (\"Opening video!\")\n",
        "    capture = cv2.VideoCapture(\"917.mp4\")\n",
        "    print (\"Video opened\\nChoosing frames\")\n",
        "    Video('917.mp4', width = 100, embed=True)\n",
        "\n",
        "    #choosing the subset of frames from which video summary will be generateed\n",
        "    frames = []\n",
        "    i=0\n",
        "    while(capture.isOpened()):\n",
        "        if i % int(sampling_rate) == 0:\n",
        "            capture.set(1,i)\n",
        "            # print i\n",
        "            ret, frame = capture.read()\n",
        "            if len(frames) == 512 or frame is None:\n",
        "                break\n",
        "            #im = np.expand_dims(im, axis=0) #convert to (1, width, height, depth)\n",
        "            # print frame.shape\n",
        "            frames.append(np.asarray(frame))\n",
        "        i+=1\n",
        "    frames = np.array(frames)#convert to (num_frames, width, height, depth)\n",
        "    print (\"Frames chosen\")\n",
        "    print (\"Length of video %d\" % frames.shape[0])\n",
        "    # REPLACE WITH APPROPRIATE FEATURES\n",
        "\n",
        "    features = get_cnn_feat(frames, frames.shape[0])\n",
        "    print (\"Shape of features \" + str(features.shape))\n",
        "\n",
        "    # clustering: defaults to using the features\n",
        "    print (\"Clustering\")\n",
        "\n",
        "    # converting percentage to actual number\n",
        "    num_centroids=int(percent*frames.shape[0]*sampling_rate/100)\n",
        "\n",
        "\t# choose number of centroids for clustering from user required frames (specified in GT folder for each video)\n",
        "    if percent == -1:\n",
        "        video_address = \"917.mp4\".split('/')\n",
        "        gt_file = video_address[len(video_address) - 1].split('.')[0] + '.mat'\n",
        "        video_address[len(video_address) - 1] = gt_file\n",
        "        video_address[len(video_address) - 2] = 'GT'\n",
        "        gt_file = '/'.join(video_address)\n",
        "        num_frames = int(scipy.io.loadmat(gt_file).get('user_score').shape[0])\n",
        "\n",
        "    if len(frames) < num_centroids:\n",
        "        print (\"Samples too less to generate such a large summary\")\n",
        "        print (\"Changing to maximum possible centroids\")\n",
        "        num_centroids=frames.shape[0]\n",
        "    #kmeans = GaussianMixture(n_components=num_centroid)\n",
        "    kmeans = KMeans(n_clusters=num_centroids)\n",
        "    print (\"Done Clustering!\")\n",
        "\n",
        "    print (\"Generating summary frames\")\n",
        "    summary_frames=[]\n",
        "\n",
        "    # transforms into cluster-distance space (n_cluster dimensional)\n",
        "    \"\"\"feature_transform= kmeans.fit_transform(features)\n",
        "    frame_indices=[]\n",
        "    for cluster in range(feature_transform.shape[1]):\n",
        "        print (\"Frame number: %d\" % (np.argmin(feature_transform.T[cluster])*sampling_rate))\n",
        "        frame_indices.append(np.argmin(feature_transform.T[cluster]))\"\"\"\n",
        "    feat = kmeans.fit_predict(features)\n",
        "    frame_indices = []\n",
        "    for cluster_center in kmeans.cluster_centers_:  # Loop over the cluster centers\n",
        "        distances_to_center = np.linalg.norm(features - cluster_center, axis=1)  # Distances to the center\n",
        "        closest_point_index = np.argmin(distances_to_center)  # Index of the closest data point\n",
        "        frame_indices.append(closest_point_index)  # Choose the frame closest to the cluster center\n",
        "\n",
        "    print(sorted(frame_indices))\n",
        "\t# frames generated in sequence from original video\n",
        "    frame_indices=sorted(frame_indices)\n",
        "    summary_frames=[frames[i] for i in frame_indices]\n",
        "    print (\"Generated summary\")\n",
        "    x = []\n",
        "    for i in frame_indices:\n",
        "        image = (torch.tensor(np.array(cv2.resize(frames[i],(224,224)))).permute(2,0,1) )/255\n",
        "        x.append(image)     \n",
        "    x = DataLoader(x,batch_size=32,shuffle=True)    \n",
        "    output = evaluate(model, x, criterion)\n",
        "    print(output)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = CrossEfficientViT(config).to(device)\n",
        "    main(model, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Input\n",
            "  Downloading input-0.0.0.tar.gz (380 bytes)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: Input\n",
            "  Building wheel for Input (setup.py): started\n",
            "  Building wheel for Input (setup.py): finished with status 'done'\n",
            "  Created wheel for Input: filename=input-0.0.0-py3-none-any.whl size=909 sha256=e9b130440db3db1611f14ec340fffb56942675c57267d1edd2ab9c87d28e5a97\n",
            "  Stored in directory: c:\\users\\sanje\\appdata\\local\\pip\\cache\\wheels\\b6\\b2\\3f\\a45204a4de7fd9888cdda5f0fae0ea6d6396ce76ab4ab14197\n",
            "Successfully built Input\n",
            "Installing collected packages: Input\n",
            "Successfully installed Input-0.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'Input'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mInput\u001b[39;00m\n\u001b[0;32m      2\u001b[0m Input(\u001b[39m\"\u001b[39m\u001b[39mEnter a number\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Input'"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10b244c9162f4423b02481097852d35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6f9baee066c426ab8eb9c5e8ee2701e",
              "IPY_MODEL_c984440391c14eb283f2dd3ed757cf86",
              "IPY_MODEL_fd5c482265b149bf8dfb11b391eaf835"
            ],
            "layout": "IPY_MODEL_d669b1bb322a4d7794e1b65769fa975b"
          }
        },
        "36bf6ac2bf234a6b8c264155ea0a5b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52757dae9b1d47e48e9728d11f0fcaf8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac0d4a8333554b53b60dd1f8f289803f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adb5f29b4fe8490c8cfb7f9f5a3ae1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c984440391c14eb283f2dd3ed757cf86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52757dae9b1d47e48e9728d11f0fcaf8",
            "max": 21388428,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6b79482b3db4ddd807db138a3ef95fd",
            "value": 21388428
          }
        },
        "d669b1bb322a4d7794e1b65769fa975b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6b79482b3db4ddd807db138a3ef95fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6f9baee066c426ab8eb9c5e8ee2701e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac0d4a8333554b53b60dd1f8f289803f",
            "placeholder": "​",
            "style": "IPY_MODEL_36bf6ac2bf234a6b8c264155ea0a5b6c",
            "value": "100%"
          }
        },
        "f9041ff714df4c2593f3c7a0eaba0254": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd5c482265b149bf8dfb11b391eaf835": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9041ff714df4c2593f3c7a0eaba0254",
            "placeholder": "​",
            "style": "IPY_MODEL_adb5f29b4fe8490c8cfb7f9f5a3ae1dc",
            "value": " 20.4M/20.4M [00:04&lt;00:00, 5.74MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
